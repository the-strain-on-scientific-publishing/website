<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>The Strain on Scientific Publishing</title>
<link>https://the-strain-on-scientific-publishing.github.io/website/post.html</link>
<atom:link href="https://the-strain-on-scientific-publishing.github.io/website/post.xml" rel="self" type="application/rss+xml"/>
<description>Home page for the paper &#39;The Strain on Scientific Publishing&#39; by Mark A Hanson, Dan Brockington, Paolo Crosetto and Pablo Gomez Barreiro</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Mon, 09 Jun 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>Springer Nature Discovers MDPI</title>
  <dc:creator>The Strain team</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="thumbn.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/thumbn.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img"></a></p>
</figure>
</div>
<p><span style="color: #8B1A1A;">Ambiguous one-word journal titles are a Multi-Disciplinary Publishing Institute (MDPI) trademark (“<a href="https://www.mdpi.com/journal/foods">Foods</a>”, “<a href="https://www.mdpi.com/journal/plants">Plants</a>”). In the spirit of “if you can’t beat ’em, join ’em”, Springer Nature has launched a series of journals, the “Discover” series, with near-identical names (<a href="https://link.springer.com/journal/44187">Discover Food</a>, <a href="https://link.springer.com/journal/44372">Discover Plants</a>). if you don’t believe us, you can try it yourself in our “Guess Who Is Who” mini-game: 👉👉</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/publi_guess/" target="_blank"> <em><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/game.jpg" alt="Click the image to play our mini-game" width="300"></em> </a></p>
<div style="text-align: center; font-size: 0.9em; margin-top: 0.5em;">
<p>Click the image above to play our mini-games!</p>
</div>
</div></div><p><span style="color: #8B1A1A;">Why? How? And let’s ask the most important question of all: who will this benefit? It’s certainly not the authors.</span></p>
<section id="springer-nature-discovers-mdpi" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="springer-nature-discovers-mdpi">Springer Nature Discovers MDPI</h2>
<p>For-profit academic publishers say they’re allies to the research community. They provide the venue to publish articles, and only ask a small, insignificant, multi-thousand dollar fee for the trouble. <em>It’s really a steal if you think about it</em>. Researchers get to promote their work, and publishers get to have profit margins rivalling Google and other big tech companies. Everybody wins!</p>
<p>Here at the strain team, we do have a concern though. There has been an unprecedented growth in published academic articles, including the new onslaught of <a href="https://deevybee.blogspot.com/2023/10/spitting-out-ai-gobbledegook-sandwich.html">AI-gobbledygook</a>. In our study “<a href="https://direct.mit.edu/qss/article/5/4/823/124269/The-strain-on-scientific-publishing">The strain on scientific publishing”</a>, we highlighted how certain Gold Open Access for-profit publishers motivate this growth. The most significant outlier by any metric was MDPI, a publisher that has been at the centre of many critiques (see: <a href="https://retractionwatch.com/2024/12/24/finland-publication-forum-will-downgrade-hundreds-of-frontiers-and-mdpi-journals/">here</a>, <a href="https://www.timeshighereducation.com/news/germany-faces-questions-over-publishing-agreement-mdpi">here</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/leap.1574">here</a>, <a href="https://www.science.org/content/article/fast-growing-open-access-journals-stripped-coveted-impact-factors">here</a>, <a href="https://english.elpais.com/science-tech/2023-06-04/a-researcher-who-publishes-a-study-every-two-days-reveals-the-darker-side-of-science.html">here</a>…).</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="TAT_strain.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Mean Turnaround Times (days) of major publishers in 2022"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/TAT_strain.png" class="img-fluid figure-img" alt="Mean Turnaround Times (days) of major publishers in 2022"></a></p>
<figcaption>Mean Turnaround Times (days) of major publishers in 2022</figcaption>
</figure>
</div>
</div></div><p>MDPI is a unique entity, attracting negative attention for their email spam to scientists inviting them to guest edit “special issues.” This has enabled systematic citation gaming across MDPI journals by unsavoury editors, leading to a farcical and extraordinarily fast peer review process, alongside rampant rates of self-citation and impact factor inflation. On the substance of the articles themselves, the lack of scientific rigour in the MDPI editorial process has led to ridiculous articles being published on “<a href="https://deevybee.blogspot.com/2025/01/tomatoes-roaming-fields-and-canaries-in.html">tomatoes roaming the fields</a>” among <a href="https://deevybee.blogspot.com/2024/08/guest-post-my-experience-as-reviewer.html">other bunk</a>. Such practices in other publishers resulted in mass article retractions and <a href="https://retractionwatch.com/2023/12/06/wiley-to-stop-using-hindawi-name-amid-18-million-revenue-decline/">the end of the Hindawi publishing brand</a>. As a result of these high profile controversies, a number of groups have taken concrete actions to curb the damage done by the model of aggressive recruitment of guest-edited article collections, including refusing to fund the publication charges of such papers (<a href="https://www.snf.ch/en/g2ICvujLDm9ZAU8d/news/the-snsf-is-no-longer-funding-open-access-articles-in-special-issues">here</a>, <a href="https://retractionwatch.com/2024/06/28/finland-group-downgrades-60-journals/">here</a> and <a href="https://blog.doaj.org/2023/11/02/new-criteria-for-special-issues/">here</a>).</p>
<p>In a healthy world, other publishers might look at the concern over MDPI and consider how to right the ship. <em>But, dear readers, we do not live in a healthy world.</em> The biggest publishing houses are complicit, and actively contribute, to the current strain (including Elsevier, Frontiers, and more). But we were shocked and dismayed <em>(ok… not shocked, just dismayed)</em> to learn that Springer Nature Portfolio has launched a “Discover” series of journals that seem to deliberately and systematically mimic the MDPI brand.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Picture1.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Springer Nature, why not just put it on a t-shirt?"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/Picture1.jpg" class="img-fluid figure-img" alt="Springer Nature, why not just put it on a t-shirt?"></a></p>
<figcaption>Springer Nature, why not just put it on a t-shirt?</figcaption>
</figure>
</div>
</div></div><p>Publicly, the Discover series journals say they’ll accept any science: <em>“Discover journals… provide a home for all research… our remit is to ensure that all research, validated by peers, has a place in a trusted imprint.”</em> And they’ll do it fast:“[X is] <em>a Discover</em> <em>journal focused on speed of submission and review, service, and integrity.</em>”In other words: <em>“we will publish, fast, anything that you send us so long as it we can call it ‘science.’ It doesn’t need to be useful, you just need to pay us thousands of dollars.”</em></p>
<p>Sound familiar? Yes, this is precisely <a href="https://danbrockington.com/2019/12/04/an-open-letter-to-mdpi-publishing/">what MDPI used to say about its role</a> in the publishing ecosystem. They wanted to publish, fast, anything that was true, under the auspice of ‘<a href="https://danbrockington.com/2019/12/04/an-open-letter-to-mdpi-publishing/">letting readers determine its significance</a>.’</p>
<p>But it gets so much worse. Springer Nature <em>Discover</em> journals <em>are literally carbon copying MDPI titles –</em> just pricing them lower to lure scientists in. Below in the Long and Boring Data Appendix we provide the full list of Discover journals, alongside their MDPI equivalents, and the APC charged in 2025. Here we just put one section – <em>Applied Sciences</em> – as an appetizer. Out of 66 <em>Discover</em> journals, 25 have <em>identical</em> names to existing MDPI journals (<a href="https://link.springer.com/journal/44370">Viruses</a> - <a href="https://www.mdpi.com/journal/viruses">Viruses</a>), 11 differ by one letter (<a href="https://link.springer.com/journal/44187">Food</a> – <a href="https://www.mdpi.com/journal/foods">Foods</a>) and 22 more have looser but clearly distinguishable similarity (<a href="https://link.springer.com/journal/10791">Computing</a> – <a href="https://www.mdpi.com/journal/computers">Computers</a>). That’s 58 copycat journal titles out of 66 for you.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Equivalence_table_Applied_Sciences.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Discover journals in the Applied Sciences section and their MDPI homonyms"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/Equivalence_table_Applied_Sciences.png" class="img-fluid figure-img" alt="Discover journals in the Applied Sciences section and their MDPI homonyms"></a></p>
<figcaption><em>Discover</em> journals in the Applied Sciences section and their MDPI homonyms</figcaption>
</figure>
</div>
<p>And, Discover, like MDPI, are now sending unsolicited mails to academics inviting them to head up special collections (aka Special Issues). We know, because we received one such, <em>generous,</em> invitation.</p>
<p>It seems that <u><strong>Springer Nature has Discovered MDPI</strong></u><strong>.</strong></p>
</section>
<section id="why-does-this-matter" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-does-this-matter">Why does this matter?</h2>
<p>Well, publishers claim to be filling a market need. While this siren call sounds sweet and inviting, the reality behind it has been much darker. The rush to produce more work in special issues has produced an <a href="https://phys.org/news/2023-11-avalanche-published-academic-articles-erode.html">avalanche of poor quality work</a>. It also matters because the one thing that MDPI had going for it was its transparency. Footling around<sup><span class="bg-warning px-1" data-bs-toggle="tooltip" title="footling: wasting time on frivolous activities. Great word.">a</span></sup> on the MDPI website, their rejection rates, turn around times, and special issue collections were easily visible. The Discover series demonstrates no such transparency, making it much harder to hold them to account. This muddies the waters of genuine research fields, and undermines public trust in science.</p>
<p>We need to remember that Springer Nature has previously demonstrated a keen interest in generating quantity, rather than quality. In 2011 it launched Scientific Reports, which has become the largest Open Access megajournal ever, overtaking PLOS One back in 2016. What is especially salient there is that PLOS One stated publicly that its editorial rigour was uncompromising, and part of the drop in its publications <a href="https://retractionwatch.com/2017/03/15/plos-one-faced-decline-submissions-new-editor-speaks/">was a lower acceptance rate</a>. PLOS One had further recruited 3000 editorial board members to handle the then <a href="https://everyone.plos.org/2021/11/26/fifteen-years-of-plos-one/">~5000 submissions per month</a>, and introduced new article triaging measures before peer review, helping to handle the immense volume of submissions while maintaining editorial quality. <a href="https://deevybee.blogspot.com/2024/10/an-open-letter-regarding-scientific.html">We cannot see evidence of similar behaviour in Scientific Reports</a>.</p>
<p><strong>And the worst thing of all?</strong> <em>This will work.</em> The Springer Nature brand will protect Discover. Springer Nature will allow <em>Discover</em> to prosper at the same time as the services it offers are an expensive distraction. Scientists will turn to <em>Discover</em> journals, as the service offers what they are looking for – a reputable open access publication for a small fee. Indeed, many already are: the <em>Discover</em> series has grown from 5 journals in 2020 to 66 in June 2025, collectively publishing just 35 papers in 2020, but a whole 2827 in 2024 – so far, we have counted 2398 papers published from January to May in 2025 alone, so this number seems likely to double.</p>
<p>Apart from the literally identical names, how similar are <em>Discover</em> journals to their MDPI twins? It’s a mixed bag.</p>
<p>In terms of growth, newly-minted <em>Discover</em> journals show the same explosive trajectory of MDPI, <em>circa</em> 2018-19. Overall, the <em>Discover</em> franchise grew by 460% between 2023 and 2024 (635% at Discover Food, 530% at Discover Environment. 1378% at Discover Agriculture – see the Long and Boring Data Appendix for more). The subset of <em>Discover</em> journals that has an Impact Factor, as a whole, also sport the same IFI<sup><span class="bg-warning px-1" data-bs-toggle="tooltip" title="Impact Factor Inflation (IFI) is the ratio of the simple, easily gamed Impact Factor to the network- and prestige-weighted Scimago Journal Rank.">b</span></sup> as the MDPI journals they took their titles from (2024 mean IFI: <em>Discover</em> = 5.6, MDPI = 5.8, while PLOS = 3.0, BioMed Central = 3.8 – and again, IFI of all <em>Discover</em> journals and their MDPI counterparts in the LBDA).</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="IFI_strain.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Mean Impact Inflation of major publishers in 2022"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/IFI_strain.png" class="img-fluid figure-img" alt="Mean Impact Inflation of major publishers in 2022"></a></p>
<figcaption>Mean Impact Inflation of major publishers in 2022</figcaption>
</figure>
</div>
</div></div><p>Still, the <em>Discover</em> series is not overly reliant on Special Issues, <em>yet.</em> After all, one of us has already been invited to guest edit one. Discover also displays turnaround times more in line with the bulk of the scientific publishing industry (2024 mean: 140 days from submission to acceptance); this is a far cry from other for-profit OA publishers known for their heavy-handed choices in the speed-accuracy trade-off (2022 mean of 37 days for MDPI and 72 days for Frontiers).</p>
<p>So, <em>Discover</em> has the same names, same business model, but not exactly (or not <em>yet</em>) the same metrics on all fronts.</p>
</section>
<section id="what-can-be-done-about-this" class="level2">
<h2 class="anchored" data-anchor-id="what-can-be-done-about-this">What can be done about this?</h2>
<p>Let’s start by collectively doing what we can:</p>
<div style="margin-left: 2em;">
<p><strong>First,</strong> think twice before sending your work to a for-profit publisher. In biological sciences there are journal lists like <a href="https://dafnee.isem-evolution.fr/">DAFNEE</a> that aggregate society and not-for-profit journals. Services like <a href="https://peercommunityin.org/">PeerCommunityIn</a> review and publish work FREE OF CHARGE, with journal sections covering the life sciences and more. Invest your APCs back into science.</p>
<p><strong>Second,</strong> IGNORE ALL REQUESTS from these publishers. Do not respond to requests to review. Do not respond to requests to head up special collections. Do not become an editor in these journals. <em>Do not subsidise their business model.<sup><span class="bg-warning px-1" data-bs-toggle="tooltip" title="Do not pass GO. Do not let them collect $200. -- Monopoly, ruining friendships since 1935.">c</span></sup></em></p>
<p><strong>Third,</strong> notify your Director of Research. Ask them to tell to their boss, and ask them to alert key funders (i.e.&nbsp;universities and research councils). Each group will find its own tolerance for action, but time and again it has been shown that change happens when the funders take action.</p>
<p><em>And, of course:</em></p>
<p><strong>Fourth</strong>, embrace the ridiculousness of the for-profit publishing landscape, try our “Springer Nature or MDPI” mini-games and let’s have a chat on the <a href="https://bsky.app/profile/hansonmark.bsky.social/feed/aaadbjpiqymi4">Bluesky</a> #ResearchIntegrity feed.</p>
</div>
<p>Let’s stop falling for the same (in this case, <em>literally</em> the same) old tricks. Let’s stop wasting money on journals we do not need.</p>
<p>~ <em>The strain team</em>.</p>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    const tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'));
    tooltipTriggerList.map(function (tooltipTriggerEl) {
      return new bootstrap.Tooltip(tooltipTriggerEl);
    });
  });
</script>
</section>
<section id="long-and-boring-data-appendix" class="level2">
<h2 class="anchored" data-anchor-id="long-and-boring-data-appendix">Long and Boring Data Appendix</h2>
<section id="all-discover-journal-and-their-mdpi-homonyms" class="level4">
<h4 class="anchored" data-anchor-id="all-discover-journal-and-their-mdpi-homonyms">All <em>Discover</em> journal and their MDPI homonyms</h4>
<p><a href="Equivalence_table_all.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/Equivalence_table_all.png" class="img-fluid"></a></p>
</section>
<section id="number-of-articles-and-recent-growth-at-discover" class="level4">
<h4 class="anchored" data-anchor-id="number-of-articles-and-recent-growth-at-discover">Number of articles and recent growth at <em>Discover</em></h4>
<p><a href="Discover_N_articles.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/Discover_N_articles.png" class="img-fluid"></a></p>
</section>
<section id="impact-factor-inflation-at-discover-and-their-mdpi-homonyms" class="level4">
<h4 class="anchored" data-anchor-id="impact-factor-inflation-at-discover-and-their-mdpi-homonyms">Impact Factor Inflation at <em>Discover</em> and their MDPI homonyms</h4>
<p><a href="Discover_MDPI_IFI.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/Discover_MDPI_IFI.png" class="img-fluid"></a></p>
</section>
<section id="turnaround-times-at-discover" class="level4">
<h4 class="anchored" data-anchor-id="turnaround-times-at-discover">Turnaround times at <em>Discover</em></h4>
<p><a href="Discover_TAT_all.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/Discover_TAT_all.png" class="img-fluid"></a></p>


</section>
</section>

 ]]></description>
  <category>SpringerNature</category>
  <category>MDPI</category>
  <category>Discovery</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/</guid>
  <pubDate>Mon, 09 Jun 2025 22:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/discover_nature/thumbn.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How much does a journal weight? A commentary on MDPI’s own study on their self-citations rates</title>
  <dc:creator>Pablo Gómez Barreiro</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/</link>
  <description><![CDATA[ 




<p>A recent <a href="https://blog.alpsp.org/2025/03/mdpi-self-citations-study-highlights.html">blog</a> published by the Association of Learned and Professional Society Publishers, written by MDPI staff Dr.&nbsp;Giulia Stefenelli and Dr.&nbsp;Enric Sayas, explored MDPI and other publishers self-citations in 2024. In line with MDPI usual transparency, they kindly included the data they used, along with the relevant code in Python.</p>
<p>Figure 1 in their blog instantly caught my attention, and my commentary on their blog is mainly around this figure and its interpretation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="mdpi_figure1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure 1, as seen in original blog. Left y-axis represents total documents per publishers in year 2024 (blue columns), while right axis shows average self-citation per publishers in 2024 (orange dots)."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/mdpi_figure1.png" class="img-fluid figure-img" alt="Figure 1, as seen in original blog. Left y-axis represents total documents per publishers in year 2024 (blue columns), while right axis shows average self-citation per publishers in 2024 (orange dots)."></a></p>
<figcaption>Figure 1, as seen in original blog. Left y-axis represents total documents per publishers in year 2024 (blue columns), while right axis shows average self-citation per publishers in 2024 (orange dots).</figcaption>
</figure>
</div>
<p>Figure 1 is easily reproducible thanks to the provided (and well-documented) script, <code>top_10.py</code>. Now, I can do a little bit of Python, but I’m less likely to make mistakes in my native language: R. I’ve teamed up with chatGPT to translate <code>top_10.py</code> into <code>top_10_PGB.R</code>, and the code necessary to replicate this commentary is available here: [<a href="https://github.com/pgomba/pgb_website/blob/main/posts/22_03_25/top_10_PGB.R">Github link</a>]. The conversion outputs same data as their Table 1 and similar graph (I took the liberty of some aesthetic changes):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_graph_with_R.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="My attempt to replicated original graph. Good enough?"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_graph_with_R.png" class="img-fluid figure-img" alt="My attempt to replicated original graph. Good enough?"></a></p>
<figcaption>My attempt to replicated original graph. Good enough?</figcaption>
</figure>
</div>
<p>Having Total Documents in this graph was masking the information conveyed by Average Self-citation rates. Here is the data for Average self citation rates with publishers rearranged by it.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_redo.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Graph showing average self-citation values (2024) from Blog’s Figure 1 by publishers arranged by rate valueverage self-citing rate values."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_redo.png" class="img-fluid figure-img" alt="Graph showing average self-citation values (2024) from Blog’s Figure 1 by publishers arranged by rate valueverage self-citing rate values."></a></p>
<figcaption>Graph showing average self-citation values (2024) from Blog’s Figure 1 by publishers arranged by rate valueverage self-citing rate values.</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="quote.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Quote from MDPI’s self assessment on self-citation rates"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/quote.png" class="img-fluid figure-img" width="300" alt="Quote from MDPI’s self assessment on self-citation rates"></a></p>
<figcaption>Quote from MDPI’s self assessment on self-citation rates</figcaption>
</figure>
</div>
</div></div><p>As discussed on the blog, MDPI ranks 6th in self-citation among the largest publishers. However, there is a major issue with how this data has been analyzed: the average self-citation rate was calculated by simply averaging each journal’s self-citation rate without accounting for the total number of publications per journal. In other words, every journal contributed equally to the average, regardless of its size.</p>
<p>Here, I present the results of the analysis when the means are weighted by the total number of documents published per journal in 2024:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_redo_mw.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Graph showing average self-citation rates (2024) after being corrected via weighted means by total number of publications by journal."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_redo_mw.png" class="img-fluid figure-img" alt="Graph showing average self-citation rates (2024) after being corrected via weighted means by total number of publications by journal."></a></p>
<figcaption>Graph showing average self-citation rates (2024) after being corrected via weighted means by total number of publications by journal.</figcaption>
</figure>
</div>
<p>Additionally, here is a summary table comparing self-citation rates before and after considering weighted means.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="table_changes.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Table showing original and corrected (weighted) self-citation rates (2024) in %, along with difference between these values"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/table_changes.PNG" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Table showing original and corrected (weighted) self-citation rates (2024) in %, along with difference between these values"></a></p>
</figure>
</div>
<figcaption>Table showing original and corrected (weighted) self-citation rates (2024) in %, along with difference between these values</figcaption>
</figure>
</div>
<p>Overall, MDPI is the most affected publisher after applying weighted means. The reanalysis of the data using weighted means moves MDPI from 6th position (with a 14% self-citation rate) to 3rd position (with a 19.7% self-citation rate). Notably, the previous table leaders, OUP and T&amp;F, remain in their respective positions with little change in their final percentages, likely due to the balance of total documents across their journals. This contrasts with the higher threshold of total documents per journal in MDPI, possibly driven by larger journals having higher levels of self citation than smaller ones. Lets find out:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_scr.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="MDPI individual journals plotted by total number of articles published in 2024 (x-axis) and self-citing rate % (y-axis"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_scr.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="MDPI individual journals plotted by total number of articles published in 2024 (x-axis) and self-citing rate % (y-axis"></a></p>
</figure>
</div>
<figcaption>MDPI individual journals plotted by total number of articles published in 2024 (x-axis) and self-citing rate % (y-axis</figcaption>
</figure>
</div>
<p>The data shows that, out of the 237 selected journals, 57 have a self-citation rate over 20%, although only 25 have more than 1,000 articles published in 2024. MDPI published 193,873 articles, of which 47% where published in journals with a self-citation rate over 20%. This % decreases down to 10.8% for number of articles published in journals with self-cite rates over 30%. Assigning each article the journal’s self-citation rate provides an alternative perspective on MDPI’s decision to plot journal frequency by average self-citation rate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="articles_self_cite.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="A different take on self-citations. Plotting frequency of articles instead of average journal self-citation"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/articles_self_cite.png" class="img-fluid figure-img" alt="A different take on self-citations. Plotting frequency of articles instead of average journal self-citation"></a></p>
<figcaption>A different take on self-citations. Plotting frequency of articles instead of average journal self-citation</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="scr_mdpi.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Original histogram with average journal self-citation"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/scr_mdpi.png" class="img-fluid figure-img" width="300" alt="Original histogram with average journal self-citation"></a></p>
<figcaption>Original histogram with average journal self-citation</figcaption>
</figure>
</div>
</div></div><p>Weighted means present a different perspective on the 2024 self-citation landscape, making it important to analyze them in a multi-publisher context. However, conclusions drawn from both the original and reinterpreted graphs still come with significant caveats:</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="figure1_publisher_self-cites-0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="2018-2021 MDPI self-citation rates from previous publication"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/figure1_publisher_self-cites-0.png" class="img-fluid figure-img" width="300" alt="2018-2021 MDPI self-citation rates from previous publication"></a></p>
<figcaption>2018-2021 MDPI self-citation rates from previous publication</figcaption>
</figure>
</div>
</div></div><ol type="1">
<li><p>The time window is limited to 2024. Temporal context is crucial, especially for understanding shifts in self-citation trends in modern publishing. A previous <a href="https://www.mdpi.com/about/announcements/2979">MDPI self-assessment on self-citations</a>, covering the period from 2018 to 2021, showed self-citation rates close to 30%.</p></li>
<li><p>Publishers have different balances of natural sciences and humanities in their coverage, and each discipline may exhibit varying self-citation rates.</p></li>
</ol>
<p>In conclusion, analyzing self-citation at the publisher level <strong>requires the use of weighted data</strong> to be truly effective, especially to avoid biases introduced by the disparity in journal sizes. I encourage MDPI (and other publishers) to try this approach in further self-analysis of their practices.</p>



 ]]></description>
  <category>R</category>
  <category>MDPI</category>
  <category>self-citations</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/</guid>
  <pubDate>Sat, 22 Mar 2025 23:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/thumbn.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Announcement: “The Strain on Scientific Publishing” data explorer!</title>
  <dc:creator>Pablo Gómez Barreiro</dc:creator>
  <dc:creator>Mark Hanson</dc:creator>
  <dc:creator>Paolo Crosetto</dc:creator>
  <dc:creator>Dan Brockington</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/thumbn.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="627" alt="The Strain Explorer app is here!"></a></p>
</figure>
</div>
<figcaption>The Strain Explorer app is here!</figcaption>
</figure>
</div>
<p>A common sentiment we have received is that it is a shame we cannot release our data. We think so too. But the legal advice about how to treat web-scraped data is clear: we cannot share the raw data<sup>1</sup>.</p>
<p>But… After many consultations, we’ve figured out exactly what data we <strong>can</strong> share, and so we’re doing just that. We’ve provided the first of some step-by-step guides for you to download the raw data for yourself (complete with screenshots!), and R code to assemble Scimago yearly data into a single dataframe. Take a look <strong>here</strong></p>
<p>But if that sounds like a lot of work, <strong>we’re building an R-based shiny app</strong> that lets you type in your journal or publisher of interest and get reports on strain added by each publisher, proportion of special issue articles, average turnaround times per journal, and impact inflation!</p>
<p>We were planning to share this during our revisions, but we’ve decided to release a sneak peek: <a href="https://pagoba.shinyapps.io/strain_explorer/"><strong>we now have a beta version of this “Strain data explorer” on our website</strong></a><strong>.</strong> As this app is in beta, we expect a few bugs. We’ve also got some data visualization ideas to develop, and some kinks to fix. We’ll be making improvements over time, so do check back for updates, and do send in improvements and suggestions on what you’d like to see it show!</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/strain_explorer.PNG" class="img-fluid"></a></p>
</div></div><p>Cheers,</p>
<p>Pablo Gómez Barreiro, Mark A. Hanson, Paolo Crosetto, and Dan Brockington</p>
<div id="callout-1" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The nitty gritty details
</div>
</div>
<div class="callout-body-container callout-body">
<p>For those interested in accessing raw data, here are technical notes and instructions on how you can assemble those data for yourself.</p>
<p><strong>Scripts</strong></p>
<p>In the file Shiny_scripts_data_release.zip there is a folder setup in an R package format. Download <a href="https://github.com/the-strain-on-scientific-publishing/website/blob/main/data/Shiny_scripts_data_release.zip">this .zip file from Github</a>, unzip the file, and open the Rproject and the main analysis script. The “Scripts” folder does not need to be touched. But you will need to follow the instructions below to populate the folders with the data needed for running the analysis.</p>
<p><strong>Data related to Figures 1 &amp; 5</strong></p>
<p>These data come from Scimago, OECD, and UNESCO. <a href="README_assemble_Scimago_OECD_UNESCO.pdf">This .pdf</a> contains step-by-step instructions on how to assemble these data for yourself.</p>
<p><strong>Data related to Figures 2, 3, and 4</strong></p>
<p>Unfortunately, these data we simply cannot share. If publishers authorize us to release the data we have on them, we could. But until then, we have to keep these data private.</p>
<p>Still, we can explain how we collected these data, and if you’re coding-savvy, you can follow along. Pablo has been writing blog posts [<a href="https://pgomba.github.io/pgb_website/posts/08_10_23/">1</a>, <a href="https://pgomba.github.io/pgb_website/posts/11_10_23/">2</a>, <a href="https://pgomba.github.io/pgb_website/posts/18_10_23/">3</a>] on how he scraped different publishers, so stay tuned for updates. The current posts already guide you through the process for example publishers. In brief: we had to take a unique strategy to scraping each publisher, based on how they built their web HTML code. This is why our study is more limited in which publishers we analyse in Figures 2, 3, and 4. But the idea is generally the same for each publisher:</p>
<ul>
<li>Feed the script a list of DOIs/addresses</li>
<li>Search the web page for lines in the html that contain the data of interest.</li>
<li>For instance, the line below can be gives the date the article was put online: “<em>meta name=”citation_online_date” content=”2018/01/23”</em>”</li>
<li>Customize the script for each data point you want to collect, for each publisher, sometimes in subgroup-specific ways…</li>
<li>Check your data very very carefully for errors. Web pages can themselves contain errors even if your script is good! For example, we got quite a few articles published online Jan 1st 1970 (the so-called “epoch date”).</li>
</ul>
<p><strong>A technical note on Impact Inflation</strong></p>
<p>In the preprint we used a 2-year window for Clarivate Impact Factor or Scopus cites/doc. However the standard calculation of Scimago Journal Rank (SJR) uses a 3-year window. As a result, changes in SJR lag changes in Impact Factor a bit. Because these are values derived from 2-3 years of citation behaviour, no single year totally throws off the calculation, but this difference in time window can introduce noise unnecessarily. We prefer to use 2-year Impact Factors and cites/doc values in the preprint as this is the most common form of Impact Factor, and so more intuitive for the reader.</p>
<p>But we can avoid this lag effect if we simply use 3-year windows for Impact Factors or cites/doc. So in the shiny app, we have used Scimago cies/doc (3-years) / SJR to calculate Impact Inflation. These numbers are very similar to the values you get from using a 2-year window for cites/doc, but for journals with big changes to cites/doc over time, the result is a slightly lower volatility in Impact Inflation from year-to-year as a result.</p>
</div>
</div>




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Researchers have a right to web-scrape information and to use it for scientific analyses, but not to further release the raw data scraped. Those data belong to the web-scraped parties (in this case, publishers), who retain rights over their distribution.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Data explorer</category>
  <category>Shiny</category>
  <category>The Strain</category>
  <category>Scientific Publishing</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/</guid>
  <pubDate>Tue, 12 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/thumbn.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Response to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”</title>
  <dc:creator>Mark Hanson</dc:creator>
  <dc:creator>Pablo Gómez Barreiro</dc:creator>
  <dc:creator>Paolo Crosetto</dc:creator>
  <dc:creator>Dan Brockington</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="thumbn2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/thumbn2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="{.lightbox}" alt="Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0"></a></p>
</figure>
</div>
<figcaption>Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0</figcaption>
</figure>
</div>
<p>We have received a diverse, global, and humbling response to our preprint “<a href="https://arxiv.org/abs/2309.15884">The Strain on Scientific Publishing</a>.” We thank everyone for the kind words, thoughtful commentaries, and critiques from across the spectrum. We embrace the opportunity to critically re-evaluate our positions and better our understanding of the data. This is Open Science at its best, and we fully support it.</p>
<p><a href="https://www.frontiersin.org/news/2024/02/21/bad-bibliometrics-dont-add-up-for-research-or-why-research-publishing-policy">A recently-published Frontiers Media “science news” blog</a> commented publicly on our work. We read this piece with great interest, but were surprised to see this blog mischaracterize our work and the relationship we had with Frontiers in the lead-up to releasing our preprint. We feel the debate around our preprint is best left to the scientific discourse, including peer review; ultimately the worth of our work to the scientific community at large will be judged by the utility that it provides to understanding the academic publishing landscape.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="frontiers_web_caption.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Web capture from Frontiers website"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/frontiers_web_caption.PNG" class="img-fluid figure-img" width="300" alt="Web capture from Frontiers website"></a></p>
<figcaption>Web capture from Frontiers website</figcaption>
</figure>
</div>
</div></div><p>However, we feel compelled to publicly reply to this blog piece as it contains factual errors, distorts our work, accuses us of editing, cutting and omitting data to produce biased results. <strong>These accusations are particularly hideous to make of scientists</strong>. Frontiers’ blog further produces an alternative analysis that partly does not stand up to scrutiny, and partly supports the very results it claims to debunk.&nbsp;</p>
<p>Here we provide a response focused on the most salient points. We must reply to the claims made about our character seriously: we did not cut, distort, omit or edit any data. Here, we will set the record straight and dispel the derogatory accusations contained in Frontiers’ piece. Without further ado:</p>
<section id="frontiers-data-claims" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="frontiers-data-claims">1. Frontiers data claims</h2>
<p><strong>First:</strong> we were surprised to see Frontiers <strong>claiming they had shared data with us</strong>. We asked them for data on several occasions between April-September 2023, and while they always responded with “we’re working on it,” they never provided us any of the data we requested on special issue articles, turnaround times, or rejection rates. <strong>So to be clear: we requested data from Frontiers such that we could validate our web scraping, but they never shared those data with us.</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>“In fact, using the same data Frontiers provided the preprint authors as well as the correct original Scopus data, we do not find any correlation between total number of articles and the number of special issues articles (Figure 2C).”</em> <br> - Frontiers</p>
</div></div><p>They did send us a personally-curated set of Dimensions data in August 2023. They did so after we shared a draft of our preprint prior to its release. Here’s a screenshot of the full extent of the data Frontiers sent us:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Frontiers_data_share.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Caption from data shared by Frontiers. Sensible information redacted"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_data_share.jpg" class="img-fluid figure-img" alt="Caption from data shared by Frontiers. Sensible information redacted"></a></p>
<figcaption>Caption from data shared by Frontiers. Sensible information redacted</figcaption>
</figure>
</div>
<p>We have redacted rows in this spreadsheet that focused on another publisher. We didn’t ask Frontiers for Dimensions data, and we are also quite capable of curating data for ourselves. We opted to use a conservative dataset where all journals in our analysis were indexed by both Scopus and Web of Science (downloaded from <a href="https://www.scimagojr.com/journalrank.php">Scimago</a>), such that the growth in articles we described would relate solely to journals indexed by both industry heavyweights. We acknowledge in our study that there are even more journals out there, for instance, journals indexed by Dimensions but not by both Scopus and Web of Science, and so the strain we describe is likely an underestimate.</p>
<p><strong>Second:</strong> Frontiers writes that we “resorted to unverifiable data obtained through web scraping”.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>“The authors resorted to unverifiable data obtained through web scraping, which means that the study and its conclusions were predicated on an incomplete and unbalanced dataset of publisher activity.”</em> <br> - Frontiers</p>
</div></div><p>For <a href="https://arxiv.org/abs/2309.15884">our preprint</a>, we scraped data on shares of special issues and turnaround times. However, our data on the number of published articles and on the number of newly-minted PhDs and world researchers – the core of Frontiers’ rebuke – were downloaded from <strong>publicly available, official data</strong> (via Scimago, OECD, and UNESCO as sources). This is not hidden but plainly stated in our preprint.</p>
<p><strong>Third:</strong> Frontiers writes “<strong>When contacted for access to their dataset</strong>, the authors responded that they had”embargoed” their data, <strong>with the result that no one can verify or replicate their findings</strong>”.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>“When contacted for access to their dataset, the authors responded that they had”embargoed” their data, with the result that no one can verify or replicate their findings.”</em> <br> - Frontiers</p>
</div></div><p>We have norecord of Frontiers requesting our dataset. Our data and scripts are uploaded to FigShare, and <a href="https://figshare.com/articles/dataset/The_strain_on_scientific_publishing_scripts_and_data_/24265726">this data deposition is even visible publicly</a>. While the data are indeed currently under embargo, the fact that they are deposited this way not does not make them unverifiable, but the opposite: it makes them accessible to journal staff and peer reviewers. Thus, this statement by Frontiers is untrue on both counts: we were not contacted for our data, and our results can indeed be replicated and verified, just not publicly.</p>
<p><strong>So why are the data under embargo?</strong> We have consulted with lawyers at our respective institutions throughout this process regarding our data management ethics and responsibilities. Unfortunately, we simply cannot release our web-scraped dataset. The reasons for this are disappointing but sensible: those articles and the metadata contained within belong to the publishers; they are not ours to redistribute. We have a right to scrape and analyse them, but not to share them further without the publishers’ authorisation. Now… if Frontiers authorised us to release the web-scraped data that we collected from them, we could do that immediately.</p>
<p><strong>But!!!</strong> While the raw data are not available, we are free to disclose analyses carried out on them. <strong>In this regard, we have an exciting announcement:</strong> we’ve just released a <a href="https://pagoba.shinyapps.io/strain_explorer/">web app (in beta)</a> that lets you explore data that we can share in a fully-customizable way! See the announcement <a href="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/"><strong>here</strong></a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/strain_explorer.PNG" class="img-fluid"></a></p>
</div></div></section>
<section id="straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said">2. Straw men, or: what we actually said vs.&nbsp;what Frontiers claims we said</h2>
<p>Frontiers claims we said many things we simply never said. We encourage readers to actually read <a href="https://arxiv.org/abs/2309.15884">our manuscript</a> to see what it is we do say. Below we give a brief summary of some of the many straw men populating Frontiers’ piece.</p>
<p><strong>Sampling of Frontiers statements:</strong></p>
<ol type="1">
<li><p>“The study posited that the scientific community is under strain due to a declining workforce and an exponential increase in published articles, caused by special issues in open access journals.”</p></li>
<li><p>“Attributing the growth of scientific output solely to gold open access publishers […].”</p></li>
<li><p>“It is clearly flawed to single out the shift of academic publishing towards open access as the sole driver of increase in scientific output […].”</p></li>
</ol>
<p><strong>Almost all of that is a straw man.</strong>&nbsp;</p>
<ul>
<li><p>We don’t say the workforce is declining, we say (in the Abstract): “Total articles… have grown exponentially… which has outpaced the limited growth – if any – in the number of practising scientists.”</p></li>
<li><p>We never attribute the growth to a single factor – the very aim of our study is to propose <strong>five</strong> indicators of strain covering different factors. Also, (from the discussion): “The strain we characterise is a complicated problem, generated by the interplay of different actors in the publishing market”.</p></li>
<li><p>We never attribute the growth solely to gold OA publishers. Instead, we highlight at least two broad models, stating (in the Discussion): “the amount of strain generated through these two strategies is comparable”; we indicate several problems over our five indicators. <strong>In fact:</strong> we openly and clearly write this (from the Discussion) “regulating behaviours cannot be done at the level of publishing model. Gold open access, for example, does not necessarily add to strain [...].”</p>
<p><strong>These are two things we do say though:</strong></p>
<ul>
<li><p>That growth in yearly published papers is exponential, and</p></li>
<li><p>That the number of researchers has not kept up with that growth</p></li>
</ul>
<p>Frontiers claims these are “false pillars of strain”, suggesting instead that the growth in articles we described is linear, and that the number of scientists over time has seen “a continuous increase.” We will give these suggestions the scrutiny they deserve.</p>
<p><strong>Linearity</strong></p>
<p><strong>First, it bears saying: “Humans tend to systematically underestimate exponential growth and perceive it in linear terms” (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1125810/full">Frontiers in Psychology, 2023</a>).</strong> With this in mind… Frontiers suggested growth in articles was linear by… saying so. While criticizing us for not performing “scientific analysis”, their analysis can be summed up as showing plots and saying “it’s linear!”</p>

<p>However, we ran the stats on this, and over our study’s time period, a linear model is a good fit (R2 = 0.93) but an exponential model is an even better fit (R2 = 0.97, below). Indeed, we reported a mean year-over-year growth of ~5.6% over the period 2016-2022. The bizarre thing is… Frontiers also annotated a 6% year-over-year growth in their Dimensions data from 2017-2022 (above: see the Dimensions data they shared).</p>
<p>Constant absolute growth is linear. Constant year-over-year percent growth is exponential. So in fact, we both agree? The data are exponential. Below we have provided plots of our data with linear or exponential curves annotated, and provided formulae and model fits for both models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="expolinear.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Who wore it better? Linear (red), or exponential (black)?"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/expolinear.png" class="img-fluid figure-img" alt="Who wore it better? Linear (red), or exponential (black)?"></a></p>
<figcaption>Who wore it better? Linear (red), or exponential (black)?</figcaption>
</figure>
</div></li>
</ul>
<div class="no-row-height column-margin column-container"><div class="">
<p><em>“Humans tend to systematically underestimate exponential growth and perceive it in linear terms, which can have severe consequences in a variety of fields.”</em> <br> - Melnik-Leroy, Gerda Ana, et al.&nbsp;“<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1125810/full">Is my visualization better than yours? Analyzing factors modulating exponential growth bias in graphs.</a>” <em>Frontiers in psychology</em> 14 (2023): 1125810.</p>
</div></div><table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 29%">
<col style="width: 10%">
<col style="width: 27%">
<col style="width: 10%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>2013-2022</strong></td>
<td><strong>R<sup>2</sup></strong></td>
<td><strong>2000-2022</strong></td>
<td><strong>R<sup>2</sup></strong></td>
</tr>
<tr class="even">
<td>Linear: ax + b</td>
<td>ax = 120379 * yr<br>
b = -240712967</td>
<td>0.93</td>
<td>a = 80319 * yr<br>
b = -159895398</td>
<td>0.96</td>
</tr>
<tr class="odd">
<td>Exponential: a<sup>bx</sup></td>
<td>a = 1.637*106<br>
bx = (5.76* 10-2) (yr - 2013)</td>
<td>0.97</td>
<td>a = 8.74*105<br>
bx = (5.12*10-2) (yr-2000)</td>
<td>0.99</td>
</tr>
</tbody>
</table>
<p><strong>Decline of the number of researchers</strong></p>
<p>Again, we never claimed a decline. From our abstract: “Total articles… have grown exponentially… which has outpaced the limited growth – if any – in the number of practising scientists.”</p>
<p>Some of our plots do show a decline in the number of new PhDs per year. A lower growth rate does not create an absolute reduction – as people all over the planet facing slower inflation this year surely know. Further, we did consider different data types and sources to pin down the growth in the global scientific workforce: some show a plateau, some show only a limited rate of growth. That’s why we said “growth in articles outpaced the limited growth – if any – of researchers”. It’s what the data (from several sources) tell us. Here is our Fig.1supp1, which uses OECD PhD data supplemented with data for India and China (A-B), or UNESCO data on researchers-per-million (C-D):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Fig1supp1_docs_to_researchers.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Fig1supp1_docs_to_researchers.png" class="img-fluid figure-img" alt="Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019."></a></p>
<figcaption>Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019.</figcaption>
</figure>
</div>
<p><strong>So, what did we actually say?</strong></p>
<p>We claim, and document, that strain is a real problem, and while it’s not a new problem, it’s become seriously overwhelming in the last few years. We explicitly discuss how <strong>two</strong> main mechanisms have generated this strain: <strong>i)</strong> A steady growth in total journals, and in articles per journal, by legacy publishers like Elsevier and Springer. <strong>ii)</strong> An explosion of articles by publishers adopting special issues as a format to publish the majority of their articles, such as MDPI and Frontiers. In our discussion, we further emphasize these two mechanisms as distinct contributors to strain, and at no point do we suggest that strain is caused by a single publisher or a single publishing behaviour.</p>
<p>Strain is caused by an industry that seeks to grow off the backs of a volunteer workforce that increasingly cannot keep up with demand to write, review, and edit new articles. Read that again and think of your life as a scientist. It resonates, right? What we did is characterize the constitution of that strain, synthesising data that are rarely considered collectively.</p>
<p><strong>Finally, the straw man to bind them all</strong></p>
<p>In their piece, Frontiers makes us out to somehow be “detractors” of the open science movement, setting out to falsely prove our pre-ordained views… while discussing our open access preprint. We are confident our record of public statements and publishing history firmly advocate for open science.</p>
</section>
<section id="of-bad-bibliometrics-that-dont-add-up" class="level2">
<h2 class="anchored" data-anchor-id="of-bad-bibliometrics-that-dont-add-up">3. Of Bad bibliometrics that don’t add up</h2>
<p>We struggled to come up with a delicate way to phrase this, but we simply couldn’t find the words. To be blunt: the Frontiers analysis is amateurish, careless in its data curation and interpretation, leans heavily on visual impressions, and evokes results out of thin air. Let’s dive in.</p>
<p><strong>Frontiers claims:</strong></p>
<p>“The proxy data was <strong>not reproducible</strong> and <strong>not representative</strong> of the original sources of Scopus and Web of Science. The original data does not show the claimed”exponential increase” in the total number of articles published during the study period (2013-2022), <strong>in fact growth is linear</strong> during this period (Figure 2A).”</p>
<p><strong>First:</strong> Frontiers presents this plot of exponential historical article growth, which suggests that the total new articles per year has declined from 2021 to 2022 slightly in Dimensions and Scopus, and by approximately half a million articles per year in the Web of Science. Here is their Figure 1:</p>
<p><a href="Figure_1_Historical_Article_Growth.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Figure_1_Historical_Article_Growth.png" class="img-fluid"></a></p>
<p>However, in their next plot in their Figure 2A (below, left), they instead show a year-over-year increase in the Dimensions and Scopus datasets, and only a minor decline in the Web of Science data. What Frontiers has done in Figure 1 is they have oversmoothed their curve, leading to a faulty data visualization that does not accurately reflect the underlying data.</p>
<p>Their Figure 2A also shows another problem with the analysis carried out by Frontiers, that will occur again later: they claim one thing in the text, provide no numbers in support, and simply reference a figure that, in fact, does not support the claim.</p>
<p>Look at the left panel below, and then the right panel. Frontiers claims that our data are “incomplete and selective” and that “employing reproducible data from original, verifiable sources painted a starkly different picture”. They then provide their Figure 2A, which we give here unaltered in the left panel below. In the right panel, we have cut and pasted the black line (our data) and moved it up so the 2013 data points are aligned between Scopus and “our data”. The growth rate is remarkably similar. This should not surprise anyone, because <strong>our data are Scopus data.</strong> We just filtered them to only include journals also indexed in Web of Science. They are <strong>not</strong> a “proxy, unverifiable source.”</p>
<p><a href="Frontiers_Fig2a.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2a.png" class="img-fluid"></a></p>
<p>But to be sure we do not somehow mislead you with a trick of data visualisation, we crunched the numbers: the correlation between our data and Scopus is: Pearson’s r = 0.992. In fact, the correlation between our data and the Dimensions data Frontiers provided us is: Pearson’s r = 0.983. So even if you use Dimensions or Scopus instead, <strong>our data do paint the same picture.</strong></p>
<p><strong>Pillars made of soapstone:</strong> another example of claiming a result, not running any analysis, and then reaching a conclusion not supported by the figure is their “second pillar of strain”: the limited growth – if any – in the number of researchers. Here is Frontiers’ claim:</p>
<p>“The study uses PhD graduates as a proxy for active researchers who write, review, and edit articles when a more direct measure of active researchers is available… They ignore the upward trend available from the same data source they used (Figure 2B) and fail to mention or consider that PhD graduations were disrupted during the pandemic. A direct measure of the number of active researchers shows the opposite to what they claim – a continuous increase.”</p>
<p>Again, we never say there has been a decrease in researchers (see above). Frontiers then provides their Figure 2B and claims it shows a continuous increase in researchers.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><a href="Frontiers_Fig2b.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2b.png" class="column img-fluid"></a></p>
</div><div class="column" style="width:5%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:55%;">
<p>But we tend to see a plateau in this plot, similar to the plateau we saw in newly-minted PhDs. And again, the reduction in newly-minted PhDs was robust to inclusion of data from India and China (see Fig.1supp1 above). We further double checked this trend using UNESCO data on researchers per million, similar to what Frontiers has done here, finding a very limited growth in recent years. That’s why we only claimed that article growth had outpaced the growth - if any - in active researchers.</p>
</div>
</div>
<p><strong>There are actually additional problems with the data that Frontiers plot here:</strong> the long timeframe they show is misleading because the countries indexed in the OECD dataset have evolved over time. Indeed, the huge spike in 2013 is due to the inclusion of many new countries, including Germany, explaining this large spike in total new researchers.</p>
<p>Further, Frontiers claims the COVID-19 pandemic is behind the plateauing of PhD graduates that we saw. However, this plateau, also seen in the total researchers data above, started in ~2017. So we can’t exactly attribute this to the pandemic. So again, none of what Frontiers claims is true: there has not been a continuous increase in total researchers, but rather the same plateau we saw in PhD graduate numbers - the agreement between these two metrics is also quite sensible, considering PhD graduates become researchers.</p>
<p><strong>Frontiers actually validates our data:</strong> this response would not be complete without a look at Figure 2C. It purports to show the absence of correlation between total articles per year being generated and the number of Frontiers articles published in “Research Topics” (i.e.&nbsp;special issues). Frontiers plots the datasets of Scopus and Web of Science as lines, and then its own Research Topic articles as very small bars, choosing two different visualization styles within the same plot.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><a href="Frontiers_Fig2c.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2c.png" class="column img-fluid"></a></p>
</div><div class="column" style="width:5%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:55%;">
<p><strong>Briefly:</strong> there is actually quite a good correlation between total articles and Frontiers Research Topic articles per year. The correlation between the Scopus data in green and the Frontiers Research Topic data is: Pearson’s r = 0.912. Moreover, the growth in Frontiers Research Topic articles has been exponential: a mean year-over-year growth of 43%, far outpacing the overall strain (~6%).</p>
<p><strong>But somewhat ironically:</strong> this plot actually validates our own scraped data from Frontiers. As we mentioned, we requested data from Frontiers on special issue article numbers so we could validate or even replace our web-scraped data, but they never sent us those data. <strong>But in this Figure 2C, in fact, Frontiers provides the data we requested.</strong> So, how did we do? It seems we were spot-on.</p>
</div>
</div>
<p>Our web-scraped dataset estimated 69.3% of articles in Frontiers were published through “Research Topics” (i.e.&nbsp;special issues). Applying this estimate to their 2022 output (<a href="https://progressreport.frontiersin.org/">125.1k articles</a>) yields ~87k special issue articles in 2022 – the Figure here says 88k. Thus, <strong>our scrape of Frontiers articles yielded ~99% accuracy</strong> – pretty impressive given we only scraped the 49 journals from our study, while the 88k reflects all 200+ Frontiers journals. Thank you to Frontiers for providing these data that validates our web-scraping.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We started a conversation with Frontiers while we were working on our article. We offered them the chance to comment on our work before we released it publicly. This is a courtesy we also extended to other publishers. We were thus surprised to find this blog posted without our knowledge. Indeed, we had numerous emails with Frontiers in the lead-up to our work that were good-faith exchanges. We even have the original comments that Frontiers provided to us from when we sent them our draft article: they praised aspects of our work and provided constructive feedback, including correcting our phrasing to avoid ambiguous wording. We would be happy to share those comments, if Frontiers would give us permission to. We still thank Frontiers for those comments.&nbsp;</p>
<p>That is why it is so surprising to see this latest piece, which contains a startling level of animosity and many derogatory accusations that are simply untrue.</p>
<p>Others, including publishers, have welcomed our scientific countribution. Frontiers could have done the same. Instead, Frontiers has released this blog containing many analytical errors just one week after they published <a href="https://www.swissinfo.ch/eng/science/wrong-ai-generated-images-in-scientific-journal-put-a-strain-on-swiss-publisher-frontiers/73657004">the questionable article on rodent genitalia</a>. Of course mistakes happen, and Frontiers retracted that article swiftly. But this? This is something else. These are their own words, chosen intentionally.&nbsp;</p>
<p>We still want to thank specific employees at Frontiers with whom we had a respectful relationship. We made every effort we could to adhere to ethical research conduct. We had good faith, mutually beneficial exchanges with Frontiers. Thus why it is so disheartening to see this blog post and the tone it took. We hope Frontiers will reflect on what they’ve said, how they’ve said it, and choose to engage with us more productively in the future.</p>
<p>We wrote the Strain paper because we wanted there to be more transparency over the data that publishers control on academic publishing; because we thought we needed a more strongly data-driven conversation about publishing trends; because some of the contributions to that conversation are misguided, dysfunctional and require rectifying, and because we think researchers need to scrutinise publishers’ data if they are to be effectively analysed and interpreted. We cannot assume publishers will do that well.</p>
<p>The Frontier’s blog confirms us in that view.</p>
<p><em>Sincerely,</em></p>
<p><em>Mark A. Hanson, Pablo Gómez Barreiro, Paolo Crosetto, Dan Brockington</em></p>


</section>

 ]]></description>
  <category>Frontiers</category>
  <category>The Strain</category>
  <category>Open Access</category>
  <category>Scientific Publishing</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/</guid>
  <pubDate>Tue, 12 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/thumbn2.png" medium="image" type="image/png" height="112" width="144"/>
</item>
</channel>
</rss>
