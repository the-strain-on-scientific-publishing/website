<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>The Strain on Scientific Publishing</title>
<link>https://the-strain-on-scientific-publishing.github.io/website/post.html</link>
<atom:link href="https://the-strain-on-scientific-publishing.github.io/website/post.xml" rel="self" type="application/rss+xml"/>
<description>Home page for the paper &#39;The Strain on Scientific Publishing&#39; by Mark A Hanson, Dan Brockington, Paolo Crosetto and Pablo Gomez Barreiro</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Sun, 08 Jun 2025 23:00:00 GMT</lastBuildDate>
<item>
  <title>Nature discovers MPDI</title>
  <dc:creator>The Strain team</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="thumbn.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/thumbn.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img"></a></p>
</figure>
</div>
<p>Ambiguous one-word journal titles are an MDPI trademark (â€œ<a href="https://www.mdpi.com/journal/foods">Foods</a>â€, â€œ<a href="https://www.mdpi.com/journal/plants">Plants</a>â€). In the spirit of â€œif you canâ€™t beat â€™em, join â€™emâ€, Springer Nature has launched a series of journals, the â€œDiscoverâ€ series, with near-identical names (<a href="https://link.springer.com/journal/44187">Discover Food</a>, <a href="https://link.springer.com/journal/44372">Discover Plants</a>). if you donâ€™t believe us, you can try it yourself in our â€œGuess Who Is Whoâ€ mini-game: ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/publi_guess/" target="_blank"> <img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/game.jpg" alt="Click the image to play our mini-game" width="300"> </a></p>
<div style="text-align: center; font-size: 0.9em; margin-top: 0.5em;">
<p>Click the image to play our mini-games</p>
</div>
</div></div><p>Why? How? And letâ€™s ask the most important question of all: who will this benefit? Itâ€™s certainly not the authors.</p>
<p>For-profit academic publishers say theyâ€™re allies to the research community. They provide the venue to publish articles, and only ask a small, insignificant, multi-thousand dollar fee for the trouble. <em>Itâ€™s really a steal if you think about it</em>. Researchers get to promote their work, and publishers get to have profit margins rivalling Google and other big tech companies. Everybody wins!</p>
<p>Here at the strain team, we do have a concern though. There has been an unprecedented growth in published academic articles, including the new onslaught of <a href="https://deevybee.blogspot.com/2023/10/spitting-out-ai-gobbledegook-sandwich.html">AI-gobbledygook</a>. In our study â€œ<a href="https://direct.mit.edu/qss/article/5/4/823/124269/The-strain-on-scientific-publishing">The strain on scientific publishingâ€</a>, we highlighted how certain Gold Open Access for-profit publishers motivate this growth. The most significant outlier by any metric was Multi-Disciplinary Publishing Institute (MDPI), a publisher that has been at the centre of many critiques (see: <a href="https://retractionwatch.com/2024/12/24/finland-publication-forum-will-downgrade-hundreds-of-frontiers-and-mdpi-journals/">here</a>, <a href="https://www.timeshighereducation.com/news/germany-faces-questions-over-publishing-agreement-mdpi">here</a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/leap.1574">here</a>, <a href="https://www.science.org/content/article/fast-growing-open-access-journals-stripped-coveted-impact-factors">here</a>, <a href="https://english.elpais.com/science-tech/2023-06-04/a-researcher-who-publishes-a-study-every-two-days-reveals-the-darker-side-of-science.html">here</a>â€¦).</p>
<p>MDPI is a unique entity, attracting negative attention for their email spam to scientists inviting them to guest edit â€œspecial issues.â€ This has enabled systematic citation gaming across MDPI journals by unsavoury editors, leading to a farcical and extraordinarily fast peer review process, alongside rampant rates of self-citation and impact factor inflation. On the substance of the articles themselves, the lack of scientific rigour in the MDPI editorial process has led to ridiculous articles being published on â€œ<a href="https://deevybee.blogspot.com/2025/01/tomatoes-roaming-fields-and-canaries-in.html">tomatoes roaming the fields</a>â€ among <a href="https://deevybee.blogspot.com/2024/08/guest-post-my-experience-as-reviewer.html">other bunk</a>. Such practices in other publishers resulted in mass article retractions and <a href="https://retractionwatch.com/2023/12/06/wiley-to-stop-using-hindawi-name-amid-18-million-revenue-decline/">the end of the Hindawi publishing brand</a>. As a result of these high profile controversies, a number of groups have taken concrete actions to curb the damage done by the model of aggressive recruitment of guest-edited article collections, including refusing to fund the publication charges of such papers [refs: SNSF, Finland, DOAJ].</p>
<p><a href="TAT_strain.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/TAT_strain.png" class="img-fluid"></a></p>
<p>In a healthy world, other publishers might look at the concern over MDPI and consider how to right the ship. <em>But, dear readers, we do not live in a healthy world.</em> The biggest publishing houses are complicit, and actively contribute, to the current strain (including Elsevier, Frontiers, and more). But we were shocked and dismayed <em>(okâ€¦ not shocked, just dismayed)</em> to learn that Springer Nature Portfolio has launched a â€œDiscoverâ€ series of journals that seem to deliberately and systematically mimic the MDPI brand.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Picture1.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Mean Turnaround Times (days) of major publishers in 2022"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/Picture1.jpg" class="img-fluid figure-img" alt="Mean Turnaround Times (days) of major publishers in 2022"></a></p>
<figcaption>Mean Turnaround Times (days) of major publishers in 2022</figcaption>
</figure>
</div>
</div></div><p>Publicly, the Discover series journals say theyâ€™ll accept any science: <em>â€œDiscover journalsâ€¦ provide a home for all researchâ€¦ our remit is to ensure that all research, validated by peers, has a place in a trusted imprint.â€</em> And theyâ€™ll do it fast:â€œ[X is] <em>a Discover</em>[PG1]&nbsp; <em>journal focused on speed of submission and review, service, and integrity.</em>â€In other words: <em>â€œwe will publish, fast, anything that you send us so long as it we can call it â€˜science.â€™ It doesnâ€™t need to be useful, you just need to pay us thousands of dollars.â€</em></p>
<p>Sound familiar? Yes, this is precisely <a href="https://danbrockington.com/2019/12/04/an-open-letter-to-mdpi-publishing/">what MDPI used to say about its role</a> in the publishing ecosystem. They wanted to publish, fast, anything that was true, under the auspice of â€˜<a href="https://danbrockington.com/2019/12/04/an-open-letter-to-mdpi-publishing/">letting readers determine its significance</a>.â€™</p>
<p>But it gets so much worse. Springer Nature <em>Discover</em> journals <em>are literally carbon copying MDPI titles â€“</em> just pricing them lower to lure scientists in. Below we provide the full list of Discover journals, alongside their MDPI equivalents, and the APC charged in 2025. Out of 66 <em>Discover</em> journals, 25 have <em>identical</em> names to existing MDPI journals (<a href="https://link.springer.com/journal/44370">Viruses</a> - <a href="https://www.mdpi.com/journal/viruses">Viruses</a>), 11 differ by one letter (<a href="https://link.springer.com/journal/44187">Food</a> â€“ <a href="https://www.mdpi.com/journal/foods">Foods</a>) and 22 more have looser but clearly distinguishable similarity (<a href="https://link.springer.com/journal/10791">Computing</a> â€“ <a href="https://www.mdpi.com/journal/computers">Computers</a>). Thatâ€™s 58 copycat journal titles out of 66 for you.</p>
<p><a href="Equivalence_table_all.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/Equivalence_table_all.png" class="img-fluid"></a></p>
<p>And, Discover, like MDPI, are now sending unsolicited mails to academics inviting them to head up special collections (aka Special Issues). We know, because we received one such, <em>generous,</em> invitation.</p>
<p>It seems that <u><strong>Springer Nature has Discovered MDPI</strong></u><strong>.</strong></p>
<p>Why does this matter? Well, publishers claim to be filling a market need. While this siren call sounds sweet and inviting, the reality behind it has been much darker. The rush to produce more work in special issues has produced an <a href="https://phys.org/news/2023-11-avalanche-published-academic-articles-erode.html">avalanche of poor quality work</a>. It also matters because the one thing that MDPI had going for it was its transparency. <span class="bg-warning px-1" data-bs-toggle="tooltip" title="â€: silly or not important. â€œHe could always do something useful instead of wasting my time with footling queries.â€ https://dictionary.cambridge.org/dictionary/english/footling">Footling</span> around on the MDPI website, their rejection rates, turn around times, and special issue collections were easily visible. The Discover series demonstrates no such transparency, making it much harder to hold them to account. This muddies the waters of genuine research fields, and undermines public trust in science.</p>



 ]]></description>
  <category>SpringerNature</category>
  <category>MDPI</category>
  <category>Discovery</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/</guid>
  <pubDate>Sun, 08 Jun 2025 23:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/discovery_nature/thumbn.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How much does a journal weight? A commentary on MDPIâ€™s own study on their self-citations rates</title>
  <dc:creator>Pablo GÃ³mez Barreiro</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/</link>
  <description><![CDATA[ 




<p>A recent <a href="https://blog.alpsp.org/2025/03/mdpi-self-citations-study-highlights.html">blog</a> published by the Association of Learned and Professional Society Publishers, written by MDPI staff Dr.&nbsp;Giulia Stefenelli and Dr.&nbsp;Enric Sayas, explored MDPI and other publishers self-citations in 2024. In line with MDPI usual transparency, they kindly included the data they used, along with the relevant code in Python.</p>
<p>Figure 1 in their blog instantly caught my attention, and my commentary on their blog is mainly around this figure and its interpretation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="mdpi_figure1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure 1, as seen in original blog. Left y-axis represents total documents per publishers in year 2024 (blue columns), while right axis shows average self-citation per publishers in 2024 (orange dots)."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/mdpi_figure1.png" class="img-fluid figure-img" alt="Figure 1, as seen in original blog. Left y-axis represents total documents per publishers in year 2024 (blue columns), while right axis shows average self-citation per publishers in 2024 (orange dots)."></a></p>
<figcaption>Figure 1, as seen in original blog. Left y-axis represents total documents per publishers in year 2024 (blue columns), while right axis shows average self-citation per publishers in 2024 (orange dots).</figcaption>
</figure>
</div>
<p>Figure 1 is easily reproducible thanks to the provided (and well-documented) script, <code>top_10.py</code>. Now, I can do a little bit of Python, but Iâ€™m less likely to make mistakes in my native language: R. Iâ€™ve teamed up with chatGPT to translate <code>top_10.py</code> into <code>top_10_PGB.R</code>, and the code necessary to replicate this commentary is available here: [<a href="https://github.com/pgomba/pgb_website/blob/main/posts/22_03_25/top_10_PGB.R">Github link</a>]. The conversion outputs same data as their Table 1 and similar graph (I took the liberty of some aesthetic changes):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_graph_with_R.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="My attempt to replicated original graph. Good enough?"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_graph_with_R.png" class="img-fluid figure-img" alt="My attempt to replicated original graph. Good enough?"></a></p>
<figcaption>My attempt to replicated original graph. Good enough?</figcaption>
</figure>
</div>
<p>Having Total Documents in this graph was masking the information conveyed by Average Self-citation rates. Here is the data for Average self citation rates with publishers rearranged by it.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_redo.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Graph showing average self-citation values (2024) from Blogâ€™s Figure 1 by publishers arranged by rate valueverage self-citing rate values."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_redo.png" class="img-fluid figure-img" alt="Graph showing average self-citation values (2024) from Blogâ€™s Figure 1 by publishers arranged by rate valueverage self-citing rate values."></a></p>
<figcaption>Graph showing average self-citation values (2024) from Blogâ€™s Figure 1 by publishers arranged by rate valueverage self-citing rate values.</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="quote.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Quote from MDPIâ€™s self assessment on self-citation rates"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/quote.png" class="img-fluid figure-img" width="300" alt="Quote from MDPIâ€™s self assessment on self-citation rates"></a></p>
<figcaption>Quote from MDPIâ€™s self assessment on self-citation rates</figcaption>
</figure>
</div>
</div></div><p>As discussed on the blog, MDPI ranks 6th in self-citation among the largest publishers. However, there is a major issue with how this data has been analyzed: the average self-citation rate was calculated by simply averaging each journalâ€™s self-citation rate without accounting for the total number of publications per journal. In other words, every journal contributed equally to the average, regardless of its size.</p>
<p>Here, I present the results of the analysis when the means are weighted by the total number of documents published per journal in 2024:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_redo_mw.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Graph showing average self-citation rates (2024) after being corrected via weighted means by total number of publications by journal."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_redo_mw.png" class="img-fluid figure-img" alt="Graph showing average self-citation rates (2024) after being corrected via weighted means by total number of publications by journal."></a></p>
<figcaption>Graph showing average self-citation rates (2024) after being corrected via weighted means by total number of publications by journal.</figcaption>
</figure>
</div>
<p>Additionally, here is a summary table comparing self-citation rates before and after considering weighted means.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="table_changes.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Table showing original and corrected (weighted) self-citation rates (2024) in %, along with difference between these values"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/table_changes.PNG" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Table showing original and corrected (weighted) self-citation rates (2024) in %, along with difference between these values"></a></p>
</figure>
</div>
<figcaption>Table showing original and corrected (weighted) self-citation rates (2024) in %, along with difference between these values</figcaption>
</figure>
</div>
<p>Overall, MDPI is the most affected publisher after applying weighted means. The reanalysis of the data using weighted means moves MDPI from 6th position (with a 14% self-citation rate) to 3rd position (with a 19.7% self-citation rate). Notably, the previous table leaders, OUP and T&amp;F, remain in their respective positions with little change in their final percentages, likely due to the balance of total documents across their journals. This contrasts with the higher threshold of total documents per journal in MDPI, possibly driven by larger journals having higher levels of self citation than smaller ones. Lets find out:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MDPI_scr.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="MDPI individual journals plotted by total number of articles published in 2024 (x-axis) and self-citing rate % (y-axis"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/MDPI_scr.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="MDPI individual journals plotted by total number of articles published in 2024 (x-axis) and self-citing rate % (y-axis"></a></p>
</figure>
</div>
<figcaption>MDPI individual journals plotted by total number of articles published in 2024 (x-axis) and self-citing rate % (y-axis</figcaption>
</figure>
</div>
<p>The data shows that, out of the 237 selected journals, 57 have a self-citation rate over 20%, although only 25 have more than 1,000 articles published in 2024. MDPI published 193,873 articles, of which 47% where published in journals with a self-citation rate over 20%. This % decreases down to 10.8% for number of articles published in journals with self-cite rates over 30%. Assigning each article the journalâ€™s self-citation rate provides an alternative perspective on MDPIâ€™s decision to plot journal frequency by average self-citation rate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="articles_self_cite.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="A different take on self-citations. Plotting frequency of articles instead of average journal self-citation"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/articles_self_cite.png" class="img-fluid figure-img" alt="A different take on self-citations. Plotting frequency of articles instead of average journal self-citation"></a></p>
<figcaption>A different take on self-citations. Plotting frequency of articles instead of average journal self-citation</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="scr_mdpi.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Original histogram with average journal self-citation"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/scr_mdpi.png" class="img-fluid figure-img" width="300" alt="Original histogram with average journal self-citation"></a></p>
<figcaption>Original histogram with average journal self-citation</figcaption>
</figure>
</div>
</div></div><p>Weighted means present a different perspective on the 2024 self-citation landscape, making it important to analyze them in a multi-publisher context. However, conclusions drawn from both the original and reinterpreted graphs still come with significant caveats:</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="figure1_publisher_self-cites-0.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="2018-2021 MDPI self-citation rates from previous publication"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/figure1_publisher_self-cites-0.png" class="img-fluid figure-img" width="300" alt="2018-2021 MDPI self-citation rates from previous publication"></a></p>
<figcaption>2018-2021 MDPI self-citation rates from previous publication</figcaption>
</figure>
</div>
</div></div><ol type="1">
<li><p>The time window is limited to 2024. Temporal context is crucial, especially for understanding shifts in self-citation trends in modern publishing. A previous <a href="https://www.mdpi.com/about/announcements/2979">MDPI self-assessment on self-citations</a>, covering the period from 2018 to 2021, showed self-citation rates close to 30%.</p></li>
<li><p>Publishers have different balances of natural sciences and humanities in their coverage, and each discipline may exhibit varying self-citation rates.</p></li>
</ol>
<p>In conclusion, analyzing self-citation at the publisher level <strong>requires the use of weighted data</strong> to be truly effective, especially to avoid biases introduced by the disparity in journal sizes. I encourage MDPI (and other publishers) to try this approach in further self-analysis of their practices.</p>



 ]]></description>
  <category>R</category>
  <category>MDPI</category>
  <category>self-citations</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/</guid>
  <pubDate>Sun, 23 Mar 2025 00:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/mdpi_scr_comment/thumbn.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Announcement: â€œThe Strain on Scientific Publishingâ€ data explorer!</title>
  <dc:creator>Pablo GÃ³mez Barreiro</dc:creator>
  <dc:creator>Mark Hanson</dc:creator>
  <dc:creator>Paolo Crosetto</dc:creator>
  <dc:creator>Dan Brockington</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/thumbn.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="627" alt="The Strain Explorer app is here!"></a></p>
</figure>
</div>
<figcaption>The Strain Explorer app is here!</figcaption>
</figure>
</div>
<p>A common sentiment we have received is that it is a shame we cannot release our data. We think so too. But the legal advice about how to treat web-scraped data is clear: we cannot share the raw data<sup>1</sup>.</p>
<p>Butâ€¦ After many consultations, weâ€™ve figured out exactly what data we <strong>can</strong> share, and so weâ€™re doing just that. Weâ€™ve provided the first of some step-by-step guides for you to download the raw data for yourself (complete with screenshots!), and R code to assemble Scimago yearly data into a single dataframe. Take a look <strong>here</strong></p>
<p>But if that sounds like a lot of work, <strong>weâ€™re building an R-based shiny app</strong> that lets you type in your journal or publisher of interest and get reports on strain added by each publisher, proportion of special issue articles, average turnaround times per journal, and impact inflation!</p>
<p>We were planning to share this during our revisions, but weâ€™ve decided to release a sneak peek: <a href="https://pagoba.shinyapps.io/strain_explorer/"><strong>we now have a beta version of this â€œStrain data explorerâ€ on our website</strong></a><strong>.</strong> As this app is in beta, we expect a few bugs. Weâ€™ve also got some data visualization ideas to develop, and some kinks to fix. Weâ€™ll be making improvements over time, so do check back for updates, and do send in improvements and suggestions on what youâ€™d like to see it show!</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/strain_explorer.PNG" class="img-fluid"></a></p>
</div></div><p>Cheers,</p>
<p>Pablo GÃ³mez Barreiro, Mark A. Hanson, Paolo Crosetto, and Dan Brockington</p>
<div id="callout-1" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The nitty gritty details
</div>
</div>
<div class="callout-body-container callout-body">
<p>For those interested in accessing raw data, here are technical notes and instructions on how you can assemble those data for yourself.</p>
<p><strong>Scripts</strong></p>
<p>In the file Shiny_scripts_data_release.zip there is a folder setup in an R package format. Download <a href="https://github.com/the-strain-on-scientific-publishing/website/blob/main/data/Shiny_scripts_data_release.zip">this .zip file from Github</a>, unzip the file, and open the Rproject and the main analysis script. The â€œScriptsâ€ folder does not need to be touched. But you will need to follow the instructions below to populate the folders with the data needed for running the analysis.</p>
<p><strong>Data related to Figures 1 &amp; 5</strong></p>
<p>These data come from Scimago, OECD, and UNESCO. <a href="README_assemble_Scimago_OECD_UNESCO.pdf">This .pdf</a> contains step-by-step instructions on how to assemble these data for yourself.</p>
<p><strong>Data related to Figures 2, 3, and 4</strong></p>
<p>Unfortunately, these data we simply cannot share. If publishers authorize us to release the data we have on them, we could. But until then, we have to keep these data private.</p>
<p>Still, we can explain how we collected these data, and if youâ€™re coding-savvy, you can follow along. Pablo has been writing blog posts [<a href="https://pgomba.github.io/pgb_website/posts/08_10_23/">1</a>, <a href="https://pgomba.github.io/pgb_website/posts/11_10_23/">2</a>, <a href="https://pgomba.github.io/pgb_website/posts/18_10_23/">3</a>] on how he scraped different publishers, so stay tuned for updates. The current posts already guide you through the process for example publishers. In brief: we had to take a unique strategy to scraping each publisher, based on how they built their web HTML code. This is why our study is more limited in which publishers we analyse in Figures 2, 3, and 4. But the idea is generally the same for each publisher:</p>
<ul>
<li>Feed the script a list of DOIs/addresses</li>
<li>Search the web page for lines in the html that contain the data of interest.</li>
<li>For instance, the line below can be gives the date the article was put online: â€œ<em>meta name=â€citation_online_dateâ€ content=â€2018/01/23â€</em>â€</li>
<li>Customize the script for each data point you want to collect, for each publisher, sometimes in subgroup-specific waysâ€¦</li>
<li>Check your data very very carefully for errors. Web pages can themselves contain errors even if your script is good! For example, we got quite a few articles published online Jan 1st 1970 (the so-called â€œepoch dateâ€).</li>
</ul>
<p><strong>A technical note on Impact Inflation</strong></p>
<p>In the preprint we used a 2-year window for Clarivate Impact Factor or Scopus cites/doc. However the standard calculation of Scimago Journal Rank (SJR) uses a 3-year window. As a result, changes in SJR lag changes in Impact Factor a bit. Because these are values derived from 2-3 years of citation behaviour, no single year totally throws off the calculation, but this difference in time window can introduce noise unnecessarily. We prefer to use 2-year Impact Factors and cites/doc values in the preprint as this is the most common form of Impact Factor, and so more intuitive for the reader.</p>
<p>But we can avoid this lag effect if we simply use 3-year windows for Impact Factors or cites/doc. So in the shiny app, we have used Scimago cies/doc (3-years) / SJR to calculate Impact Inflation. These numbers are very similar to the values you get from using a 2-year window for cites/doc, but for journals with big changes to cites/doc over time, the result is a slightly lower volatility in Impact Inflation from year-to-year as a result.</p>
</div>
</div>




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Researchers have a right to web-scrape information and to use it for scientific analyses, but not to further release the raw data scraped. Those data belong to the web-scraped parties (in this case, publishers), who retain rights over their distribution.â†©ï¸</p></li>
</ol>
</section></div> ]]></description>
  <category>Data explorer</category>
  <category>Shiny</category>
  <category>The Strain</category>
  <category>Scientific Publishing</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/</guid>
  <pubDate>Wed, 13 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/thumbn.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>Response to: â€œBad bibliometrics donâ€™t add up for research or why research publishing policy needs sound scienceâ€</title>
  <dc:creator>Mark Hanson</dc:creator>
  <dc:creator>Pablo GÃ³mez Barreiro</dc:creator>
  <dc:creator>Paolo Crosetto</dc:creator>
  <dc:creator>Dan Brockington</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="thumbn2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/thumbn2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="{.lightbox}" alt="Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0"></a></p>
</figure>
</div>
<figcaption>Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0</figcaption>
</figure>
</div>
<p>We have received a diverse, global, and humbling response to our preprint â€œ<a href="https://arxiv.org/abs/2309.15884">The Strain on Scientific Publishing</a>.â€ We thank everyone for the kind words, thoughtful commentaries, and critiques from across the spectrum. We embrace the opportunity to critically re-evaluate our positions and better our understanding of the data. This is Open Science at its best, and we fully support it.</p>
<p><a href="https://www.frontiersin.org/news/2024/02/21/bad-bibliometrics-dont-add-up-for-research-or-why-research-publishing-policy">A recently-published Frontiers Media â€œscience newsâ€ blog</a> commented publicly on our work. We read this piece with great interest, but were surprised to see this blog mischaracterize our work and the relationship we had with Frontiers in the lead-up to releasing our preprint. We feel the debate around our preprint is best left to the scientific discourse, including peer review; ultimately the worth of our work to the scientific community at large will be judged by the utility that it provides to understanding the academic publishing landscape.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="frontiers_web_caption.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Web capture from Frontiers website"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/frontiers_web_caption.PNG" class="img-fluid figure-img" width="300" alt="Web capture from Frontiers website"></a></p>
<figcaption>Web capture from Frontiers website</figcaption>
</figure>
</div>
</div></div><p>However, we feel compelled to publicly reply to this blog piece as it contains factual errors, distorts our work, accuses us of editing, cutting and omitting data to produce biased results. <strong>These accusations are particularly hideous to make of scientists</strong>. Frontiersâ€™ blog further produces an alternative analysis that partly does not stand up to scrutiny, and partly supports the very results it claims to debunk.&nbsp;</p>
<p>Here we provide a response focused on the most salient points. We must reply to the claims made about our character seriously: we did not cut, distort, omit or edit any data. Here, we will set the record straight and dispel the derogatory accusations contained in Frontiersâ€™ piece. Without further ado:</p>
<section id="frontiers-data-claims" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="frontiers-data-claims">1. Frontiers data claims</h2>
<p><strong>First:</strong> we were surprised to see Frontiers <strong>claiming they had shared data with us</strong>. We asked them for data on several occasions between April-September 2023, and while they always responded with â€œweâ€™re working on it,â€ they never provided us any of the data we requested on special issue articles, turnaround times, or rejection rates. <strong>So to be clear: we requested data from Frontiers such that we could validate our web scraping, but they never shared those data with us.</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>â€œIn fact, using the same data Frontiers provided the preprint authors as well as the correct original Scopus data, we do not find any correlation between total number of articles and the number of special issues articles (Figure 2C).â€</em> <br> - Frontiers</p>
</div></div><p>They did send us a personally-curated set of Dimensions data in August 2023. They did so after we shared a draft of our preprint prior to its release. Hereâ€™s a screenshot of the full extent of the data Frontiers sent us:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Frontiers_data_share.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Caption from data shared by Frontiers. Sensible information redacted"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_data_share.jpg" class="img-fluid figure-img" alt="Caption from data shared by Frontiers. Sensible information redacted"></a></p>
<figcaption>Caption from data shared by Frontiers. Sensible information redacted</figcaption>
</figure>
</div>
<p>We have redacted rows in this spreadsheet that focused on another publisher. We didnâ€™t ask Frontiers for Dimensions data, and we are also quite capable of curating data for ourselves. We opted to use a conservative dataset where all journals in our analysis were indexed by both Scopus and Web of Science (downloaded from <a href="https://www.scimagojr.com/journalrank.php">Scimago</a>), such that the growth in articles we described would relate solely to journals indexed by both industry heavyweights. We acknowledge in our study that there are even more journals out there, for instance, journals indexed by Dimensions but not by both Scopus and Web of Science, and so the strain we describe is likely an underestimate.</p>
<p><strong>Second:</strong> Frontiers writes that we â€œresorted to unverifiable data obtained through web scrapingâ€.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>â€œThe authors resorted to unverifiable data obtained through web scraping, which means that the study and its conclusions were predicated on an incomplete and unbalanced dataset of publisher activity.â€</em> <br> - Frontiers</p>
</div></div><p>For <a href="https://arxiv.org/abs/2309.15884">our preprint</a>, we scraped data on shares of special issues and turnaround times. However, our data on the number of published articles and on the number of newly-minted PhDs and world researchers â€“ the core of Frontiersâ€™ rebuke â€“ were downloaded from <strong>publicly available, official data</strong> (via Scimago, OECD, and UNESCO as sources). This is not hidden but plainly stated in our preprint.</p>
<p><strong>Third:</strong> Frontiers writes â€œ<strong>When contacted for access to their dataset</strong>, the authors responded that they hadâ€embargoedâ€ their data, <strong>with the result that no one can verify or replicate their findings</strong>â€.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>â€œWhen contacted for access to their dataset, the authors responded that they hadâ€embargoedâ€ their data, with the result that no one can verify or replicate their findings.â€</em> <br> - Frontiers</p>
</div></div><p>We have norecord of Frontiers requesting our dataset. Our data and scripts are uploaded to FigShare, and <a href="https://figshare.com/articles/dataset/The_strain_on_scientific_publishing_scripts_and_data_/24265726">this data deposition is even visible publicly</a>. While the data are indeed currently under embargo, the fact that they are deposited this way not does not make them unverifiable, but the opposite: it makes them accessible to journal staff and peer reviewers. Thus, this statement by Frontiers is untrue on both counts: we were not contacted for our data, and our results can indeed be replicated and verified, just not publicly.</p>
<p><strong>So why are the data under embargo?</strong> We have consulted with lawyers at our respective institutions throughout this process regarding our data management ethics and responsibilities. Unfortunately, we simply cannot release our web-scraped dataset. The reasons for this are disappointing but sensible: those articles and the metadata contained within belong to the publishers; they are not ours to redistribute. We have a right to scrape and analyse them, but not to share them further without the publishersâ€™ authorisation. Nowâ€¦ if Frontiers authorised us to release the web-scraped data that we collected from them, we could do that immediately.</p>
<p><strong>But!!!</strong> While the raw data are not available, we are free to disclose analyses carried out on them. <strong>In this regard, we have an exciting announcement:</strong> weâ€™ve just released a <a href="https://pagoba.shinyapps.io/strain_explorer/">web app (in beta)</a> that lets you explore data that we can share in a fully-customizable way! See the announcement <a href="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/"><strong>here</strong></a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/strain_explorer.PNG" class="img-fluid"></a></p>
</div></div></section>
<section id="straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said">2. Straw men, or: what we actually said vs.&nbsp;what Frontiers claims we said</h2>
<p>Frontiers claims we said many things we simply never said. We encourage readers to actually read <a href="https://arxiv.org/abs/2309.15884">our manuscript</a> to see what it is we do say. Below we give a brief summary of some of the many straw men populating Frontiersâ€™ piece.</p>
<p><strong>Sampling of Frontiers statements:</strong></p>
<ol type="1">
<li><p>â€œThe study posited that the scientific community is under strain due to a declining workforce and an exponential increase in published articles, caused by special issues in open access journals.â€</p></li>
<li><p>â€œAttributing the growth of scientific output solely to gold open access publishers [â€¦].â€</p></li>
<li><p>â€œIt is clearly flawed to single out the shift of academic publishing towards open access as the sole driver of increase in scientific output [â€¦].â€</p></li>
</ol>
<p><strong>Almost all of that is a straw man.</strong>&nbsp;</p>
<ul>
<li><p>We donâ€™t say the workforce is declining, we say (in the Abstract): â€œTotal articlesâ€¦ have grown exponentiallyâ€¦ which has outpaced the limited growth â€“ if any â€“ in the number of practising scientists.â€</p></li>
<li><p>We never attribute the growth to a single factor â€“ the very aim of our study is to propose <strong>five</strong> indicators of strain covering different factors. Also, (from the discussion): â€œThe strain we characterise is a complicated problem, generated by the interplay of different actors in the publishing marketâ€.</p></li>
<li><p>We never attribute the growth solely to gold OA publishers. Instead, we highlight at least two broad models, stating (in the Discussion): â€œthe amount of strain generated through these two strategies is comparableâ€; we indicate several problems over our five indicators. <strong>In fact:</strong> we openly and clearly write this (from the Discussion) â€œregulating behaviours cannot be done at the level of publishing model. Gold open access, for example, does not necessarily add to strain [...].â€</p>
<p><strong>These are two things we do say though:</strong></p>
<ul>
<li><p>That growth in yearly published papers is exponential, and</p></li>
<li><p>That the number of researchers has not kept up with that growth</p></li>
</ul>
<p>Frontiers claims these are â€œfalse pillars of strainâ€, suggesting instead that the growth in articles we described is linear, and that the number of scientists over time has seen â€œa continuous increase.â€ We will give these suggestions the scrutiny they deserve.</p>
<p><strong>Linearity</strong></p>
<p><strong>First, it bears saying: â€œHumans tend to systematically underestimate exponential growth and perceive it in linear termsâ€ (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1125810/full">Frontiers in Psychology, 2023</a>).</strong> With this in mindâ€¦ Frontiers suggested growth in articles was linear byâ€¦ saying so. While criticizing us for not performing â€œscientific analysisâ€, their analysis can be summed up as showing plots and saying â€œitâ€™s linear!â€</p>

<p>However, we ran the stats on this, and over our studyâ€™s time period, a linear model is a good fit (R2 = 0.93) but an exponential model is an even better fit (R2 = 0.97, below). Indeed, we reported a mean year-over-year growth of ~5.6% over the period 2016-2022. The bizarre thing isâ€¦ Frontiers also annotated a 6% year-over-year growth in their Dimensions data from 2017-2022 (above: see the Dimensions data they shared).</p>
<p>Constant absolute growth is linear. Constant year-over-year percent growth is exponential. So in fact, we both agree? The data are exponential. Below we have provided plots of our data with linear or exponential curves annotated, and provided formulae and model fits for both models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="expolinear.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Who wore it better? Linear (red), or exponential (black)?"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/expolinear.png" class="img-fluid figure-img" alt="Who wore it better? Linear (red), or exponential (black)?"></a></p>
<figcaption>Who wore it better? Linear (red), or exponential (black)?</figcaption>
</figure>
</div></li>
</ul>
<div class="no-row-height column-margin column-container"><div class="">
<p><em>â€œHumans tend to systematically underestimate exponential growth and perceive it in linear terms, which can have severe consequences in a variety of fields.â€</em> <br> - Melnik-Leroy, Gerda Ana, et al.&nbsp;â€œ<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1125810/full">Is my visualization better than yours? Analyzing factors modulating exponential growth bias in graphs.</a>â€ <em>Frontiers in psychology</em> 14 (2023): 1125810.</p>
</div></div><table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 29%">
<col style="width: 10%">
<col style="width: 27%">
<col style="width: 10%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>2013-2022</strong></td>
<td><strong>R<sup>2</sup></strong></td>
<td><strong>2000-2022</strong></td>
<td><strong>R<sup>2</sup></strong></td>
</tr>
<tr class="even">
<td>Linear: ax + b</td>
<td>ax = 120379 * yr<br>
b = -240712967</td>
<td>0.93</td>
<td>a = 80319 * yr<br>
b = -159895398</td>
<td>0.96</td>
</tr>
<tr class="odd">
<td>Exponential: a<sup>bx</sup></td>
<td>a = 1.637*106<br>
bx = (5.76* 10-2) (yr - 2013)</td>
<td>0.97</td>
<td>a = 8.74*105<br>
bx = (5.12*10-2) (yr-2000)</td>
<td>0.99</td>
</tr>
</tbody>
</table>
<p><strong>Decline of the number of researchers</strong></p>
<p>Again, we never claimed a decline. From our abstract: â€œTotal articlesâ€¦ have grown exponentiallyâ€¦ which has outpaced the limited growth â€“ if any â€“ in the number of practising scientists.â€</p>
<p>Some of our plots do show a decline in the number of new PhDs per year. A lower growth rate does not create an absolute reduction â€“ as people all over the planet facing slower inflation this year surely know. Further, we did consider different data types and sources to pin down the growth in the global scientific workforce: some show a plateau, some show only a limited rate of growth. Thatâ€™s why we said â€œgrowth in articles outpaced the limited growth â€“ if any â€“ of researchersâ€. Itâ€™s what the data (from several sources) tell us. Here is our Fig.1supp1, which uses OECD PhD data supplemented with data for India and China (A-B), or UNESCO data on researchers-per-million (C-D):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Fig1supp1_docs_to_researchers.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Fig1supp1_docs_to_researchers.png" class="img-fluid figure-img" alt="Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019."></a></p>
<figcaption>Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019.</figcaption>
</figure>
</div>
<p><strong>So, what did we actually say?</strong></p>
<p>We claim, and document, that strain is a real problem, and while itâ€™s not a new problem, itâ€™s become seriously overwhelming in the last few years. We explicitly discuss how <strong>two</strong> main mechanisms have generated this strain: <strong>i)</strong> A steady growth in total journals, and in articles per journal, by legacy publishers like Elsevier and Springer. <strong>ii)</strong> An explosion of articles by publishers adopting special issues as a format to publish the majority of their articles, such as MDPI and Frontiers. In our discussion, we further emphasize these two mechanisms as distinct contributors to strain, and at no point do we suggest that strain is caused by a single publisher or a single publishing behaviour.</p>
<p>Strain is caused by an industry that seeks to grow off the backs of a volunteer workforce that increasingly cannot keep up with demand to write, review, and edit new articles. Read that again and think of your life as a scientist. It resonates, right? What we did is characterize the constitution of that strain, synthesising data that are rarely considered collectively.</p>
<p><strong>Finally, the straw man to bind them all</strong></p>
<p>In their piece, Frontiers makes us out to somehow be â€œdetractorsâ€ of the open science movement, setting out to falsely prove our pre-ordained viewsâ€¦ while discussing our open access preprint. We are confident our record of public statements and publishing history firmly advocate for open science.</p>
</section>
<section id="of-bad-bibliometrics-that-dont-add-up" class="level2">
<h2 class="anchored" data-anchor-id="of-bad-bibliometrics-that-dont-add-up">3. Of Bad bibliometrics that donâ€™t add up</h2>
<p>We struggled to come up with a delicate way to phrase this, but we simply couldnâ€™t find the words. To be blunt: the Frontiers analysis is amateurish, careless in its data curation and interpretation, leans heavily on visual impressions, and evokes results out of thin air. Letâ€™s dive in.</p>
<p><strong>Frontiers claims:</strong></p>
<p>â€œThe proxy data was <strong>not reproducible</strong> and <strong>not representative</strong> of the original sources of Scopus and Web of Science. The original data does not show the claimedâ€exponential increaseâ€ in the total number of articles published during the study period (2013-2022), <strong>in fact growth is linear</strong> during this period (Figure 2A).â€</p>
<p><strong>First:</strong> Frontiers presents this plot of exponential historical article growth, which suggests that the total new articles per year has declined from 2021 to 2022 slightly in Dimensions and Scopus, and by approximately half a million articles per year in the Web of Science. Here is their Figure 1:</p>
<p><a href="Figure_1_Historical_Article_Growth.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Figure_1_Historical_Article_Growth.png" class="img-fluid"></a></p>
<p>However, in their next plot in their Figure 2A (below, left), they instead show a year-over-year increase in the Dimensions and Scopus datasets, and only a minor decline in the Web of Science data. What Frontiers has done in Figure 1 is they have oversmoothed their curve, leading to a faulty data visualization that does not accurately reflect the underlying data.</p>
<p>Their Figure 2A also shows another problem with the analysis carried out by Frontiers, that will occur again later: they claim one thing in the text, provide no numbers in support, and simply reference a figure that, in fact, does not support the claim.</p>
<p>Look at the left panel below, and then the right panel. Frontiers claims that our data are â€œincomplete and selectiveâ€ and that â€œemploying reproducible data from original, verifiable sources painted a starkly different pictureâ€. They then provide their Figure 2A, which we give here unaltered in the left panel below. In the right panel, we have cut and pasted the black line (our data) and moved it up so the 2013 data points are aligned between Scopus and â€œour dataâ€. The growth rate is remarkably similar. This should not surprise anyone, because <strong>our data are Scopus data.</strong> We just filtered them to only include journals also indexed in Web of Science. They are <strong>not</strong> a â€œproxy, unverifiable source.â€</p>
<p><a href="Frontiers_Fig2a.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2a.png" class="img-fluid"></a></p>
<p>But to be sure we do not somehow mislead you with a trick of data visualisation, we crunched the numbers: the correlation between our data and Scopus is: Pearsonâ€™s r = 0.992. In fact, the correlation between our data and the Dimensions data Frontiers provided us is: Pearsonâ€™s r = 0.983. So even if you use Dimensions or Scopus instead, <strong>our data do paint the same picture.</strong></p>
<p><strong>Pillars made of soapstone:</strong> another example of claiming a result, not running any analysis, and then reaching a conclusion not supported by the figure is their â€œsecond pillar of strainâ€: the limited growth â€“ if any â€“ in the number of researchers. Here is Frontiersâ€™ claim:</p>
<p>â€œThe study uses PhD graduates as a proxy for active researchers who write, review, and edit articles when a more direct measure of active researchers is availableâ€¦ They ignore the upward trend available from the same data source they used (Figure 2B) and fail to mention or consider that PhD graduations were disrupted during the pandemic. A direct measure of the number of active researchers shows the opposite to what they claim â€“ a continuous increase.â€</p>
<p>Again, we never say there has been a decrease in researchers (see above). Frontiers then provides their Figure 2B and claims it shows a continuous increase in researchers.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><a href="Frontiers_Fig2b.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2b.png" class="column img-fluid"></a></p>
</div><div class="column" style="width:5%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:55%;">
<p>But we tend to see a plateau in this plot, similar to the plateau we saw in newly-minted PhDs. And again, the reduction in newly-minted PhDs was robust to inclusion of data from India and China (see Fig.1supp1 above). We further double checked this trend using UNESCO data on researchers per million, similar to what Frontiers has done here, finding a very limited growth in recent years. Thatâ€™s why we only claimed that article growth had outpaced the growth - if any - in active researchers.</p>
</div>
</div>
<p><strong>There are actually additional problems with the data that Frontiers plot here:</strong> the long timeframe they show is misleading because the countries indexed in the OECD dataset have evolved over time. Indeed, the huge spike in 2013 is due to the inclusion of many new countries, including Germany, explaining this large spike in total new researchers.</p>
<p>Further, Frontiers claims the COVID-19 pandemic is behind the plateauing of PhD graduates that we saw. However, this plateau, also seen in the total researchers data above, started in ~2017. So we canâ€™t exactly attribute this to the pandemic. So again, none of what Frontiers claims is true: there has not been a continuous increase in total researchers, but rather the same plateau we saw in PhD graduate numbers - the agreement between these two metrics is also quite sensible, considering PhD graduates become researchers.</p>
<p><strong>Frontiers actually validates our data:</strong> this response would not be complete without a look at Figure 2C. It purports to show the absence of correlation between total articles per year being generated and the number of Frontiers articles published in â€œResearch Topicsâ€ (i.e.&nbsp;special issues). Frontiers plots the datasets of Scopus and Web of Science as lines, and then its own Research Topic articles as very small bars, choosing two different visualization styles within the same plot.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><a href="Frontiers_Fig2c.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2c.png" class="column img-fluid"></a></p>
</div><div class="column" style="width:5%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:55%;">
<p><strong>Briefly:</strong> there is actually quite a good correlation between total articles and Frontiers Research Topic articles per year. The correlation between the Scopus data in green and the Frontiers Research Topic data is: Pearsonâ€™s r = 0.912. Moreover, the growth in Frontiers Research Topic articles has been exponential: a mean year-over-year growth of 43%, far outpacing the overall strain (~6%).</p>
<p><strong>But somewhat ironically:</strong> this plot actually validates our own scraped data from Frontiers. As we mentioned, we requested data from Frontiers on special issue article numbers so we could validate or even replace our web-scraped data, but they never sent us those data. <strong>But in this Figure 2C, in fact, Frontiers provides the data we requested.</strong> So, how did we do? It seems we were spot-on.</p>
</div>
</div>
<p>Our web-scraped dataset estimated 69.3% of articles in Frontiers were published through â€œResearch Topicsâ€ (i.e.&nbsp;special issues). Applying this estimate to their 2022 output (<a href="https://progressreport.frontiersin.org/">125.1k articles</a>) yields ~87k special issue articles in 2022 â€“ the Figure here says 88k. Thus, <strong>our scrape of Frontiers articles yielded ~99% accuracy</strong> â€“ pretty impressive given we only scraped the 49 journals from our study, while the 88k reflects all 200+ Frontiers journals. Thank you to Frontiers for providing these data that validates our web-scraping.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We started a conversation with Frontiers while we were working on our article. We offered them the chance to comment on our work before we released it publicly. This is a courtesy we also extended to other publishers. We were thus surprised to find this blog posted without our knowledge. Indeed, we had numerous emails with Frontiers in the lead-up to our work that were good-faith exchanges. We even have the original comments that Frontiers provided to us from when we sent them our draft article: they praised aspects of our work and provided constructive feedback, including correcting our phrasing to avoid ambiguous wording. We would be happy to share those comments, if Frontiers would give us permission to. We still thank Frontiers for those comments.&nbsp;</p>
<p>That is why it is so surprising to see this latest piece, which contains a startling level of animosity and many derogatory accusations that are simply untrue.</p>
<p>Others, including publishers, have welcomed our scientific countribution. Frontiers could have done the same. Instead, Frontiers has released this blog containing many analytical errors just one week after they published <a href="https://www.swissinfo.ch/eng/science/wrong-ai-generated-images-in-scientific-journal-put-a-strain-on-swiss-publisher-frontiers/73657004">the questionable article on rodent genitalia</a>. Of course mistakes happen, and Frontiers retracted that article swiftly. But this? This is something else. These are their own words, chosen intentionally.&nbsp;</p>
<p>We still want to thank specific employees at Frontiers with whom we had a respectful relationship. We made every effort we could to adhere to ethical research conduct. We had good faith, mutually beneficial exchanges with Frontiers. Thus why it is so disheartening to see this blog post and the tone it took. We hope Frontiers will reflect on what theyâ€™ve said, how theyâ€™ve said it, and choose to engage with us more productively in the future.</p>
<p>We wrote the Strain paper because we wanted there to be more transparency over the data that publishers control on academic publishing; because we thought we needed a more strongly data-driven conversation about publishing trends; because some of the contributions to that conversation are misguided, dysfunctional and require rectifying, and because we think researchers need to scrutinise publishersâ€™ data if they are to be effectively analysed and interpreted. We cannot assume publishers will do that well.</p>
<p>The Frontierâ€™s blog confirms us in that view.</p>
<p><em>Sincerely,</em></p>
<p><em>Mark A. Hanson, Pablo GÃ³mez Barreiro, Paolo Crosetto, Dan Brockington</em></p>


</section>

 ]]></description>
  <category>Frontiers</category>
  <category>The Strain</category>
  <category>Open Access</category>
  <category>Scientific Publishing</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/</guid>
  <pubDate>Wed, 13 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/thumbn2.png" medium="image" type="image/png" height="112" width="144"/>
</item>
</channel>
</rss>
