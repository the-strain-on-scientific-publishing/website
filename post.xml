<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>The Strain on Scientific Publishing</title>
<link>https://the-strain-on-scientific-publishing.github.io/website/post.html</link>
<atom:link href="https://the-strain-on-scientific-publishing.github.io/website/post.xml" rel="self" type="application/rss+xml"/>
<description>Home page for the paper &#39;The Strain on Scientific Publishing&#39; by Mark A Hanson, Dan Brockington, Paolo Crosetto and Pablo Gomez Barreiro</description>
<generator>quarto-1.4.551</generator>
<lastBuildDate>Wed, 13 Mar 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Response to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”</title>
  <dc:creator>Mark Hanson</dc:creator>
  <dc:creator>Pablo Gómez Barreiro</dc:creator>
  <dc:creator>Paolo Crosetto</dc:creator>
  <dc:creator>Dan Brockington</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="thumbn2.png" class="lightbox" data-glightbox="description: .lightbox-desc-1" data-gallery="quarto-lightbox-gallery-1" title="Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/thumbn2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="{.lightbox}" alt="Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0"></a></p>
</figure>
</div>
<figcaption>Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0</figcaption>
</figure>
</div>
<p>We have received a diverse, global, and humbling response to our preprint “<a href="https://arxiv.org/abs/2309.15884">The Strain on Scientific Publishing</a>.” We thank everyone for the kind words, thoughtful commentaries, and critiques from across the spectrum. We embrace the opportunity to critically re-evaluate our positions and better our understanding of the data. This is Open Science at its best, and we fully support it.</p>
<p><a href="https://www.frontiersin.org/news/2024/02/21/bad-bibliometrics-dont-add-up-for-research-or-why-research-publishing-policy">A recently-published Frontiers Media “science news” blog</a> commented publicly on our work. We read this piece with great interest, but were surprised to see this blog mischaracterize our work and the relationship we had with Frontiers in the lead-up to releasing our preprint. We feel the debate around our preprint is best left to the scientific discourse, including peer review; ultimately the worth of our work to the scientific community at large will be judged by the utility that it provides to understanding the academic publishing landscape.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="frontiers_web_caption.PNG" class="lightbox" data-glightbox="description: .lightbox-desc-2" data-gallery="quarto-lightbox-gallery-2" title="Web capture from Frontiers website"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/frontiers_web_caption.PNG" class="img-fluid figure-img" width="300" alt="Web capture from Frontiers website"></a></p>
<figcaption>Web capture from Frontiers website</figcaption>
</figure>
</div>
</div></div><p>However, we feel compelled to publicly reply to this blog piece as it contains factual errors, distorts our work, accuses us of editing, cutting and omitting data to produce biased results. <strong>These accusations are particularly hideous to make of scientists</strong>. Frontiers’ blog further produces an alternative analysis that partly does not stand up to scrutiny, and partly supports the very results it claims to debunk.&nbsp;</p>
<p>Here we provide a response focused on the most salient points. We must reply to the claims made about our character seriously: we did not cut, distort, omit or edit any data. Here, we will set the record straight and dispel the derogatory accusations contained in Frontiers’ piece. Without further ado:</p>
<section id="frontiers-data-claims" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="frontiers-data-claims">1. Frontiers data claims</h2>
<p><strong>First:</strong> we were surprised to see Frontiers <strong>claiming they had shared data with us</strong>. We asked them for data on several occasions between April-September 2023, and while they always responded with “we’re working on it,” they never provided us any of the data we requested on special issue articles, turnaround times, or rejection rates. <strong>So to be clear: we requested data from Frontiers such that we could validate our web scraping, but they never shared those data with us.</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>“In fact, using the same data Frontiers provided the preprint authors as well as the correct original Scopus data, we do not find any correlation between total number of articles and the number of special issues articles (Figure 2C).”</em> <br> - Frontiers</p>
</div></div><p>They did send us a personally-curated set of Dimensions data in August 2023. They did so after we shared a draft of our preprint prior to its release. Here’s a screenshot of the full extent of the data Frontiers sent us:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Frontiers_data_share.jpg" class="lightbox" data-glightbox="description: .lightbox-desc-3" data-gallery="quarto-lightbox-gallery-3" title="Caption from data shared by Frontiers. Sensible information redacted"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_data_share.jpg" class="img-fluid figure-img" alt="Caption from data shared by Frontiers. Sensible information redacted"></a></p>
<figcaption>Caption from data shared by Frontiers. Sensible information redacted</figcaption>
</figure>
</div>
<p>We have redacted rows in this spreadsheet that focused on another publisher. We didn’t ask Frontiers for Dimensions data, and we are also quite capable of curating data for ourselves. We opted to use a conservative dataset where all journals in our analysis were indexed by both Scopus and Web of Science (downloaded from <a href="https://www.scimagojr.com/journalrank.php">Scimago</a>), such that the growth in articles we described would relate solely to journals indexed by both industry heavyweights. We acknowledge in our study that there are even more journals out there, for instance, journals indexed by Dimensions but not by both Scopus and Web of Science, and so the strain we describe is likely an underestimate.</p>
<p><strong>Second:</strong> Frontiers writes that we “resorted to unverifiable data obtained through web scraping”.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>“The authors resorted to unverifiable data obtained through web scraping, which means that the study and its conclusions were predicated on an incomplete and unbalanced dataset of publisher activity.”</em> <br> - Frontiers</p>
</div></div><p>For <a href="https://arxiv.org/abs/2309.15884">our preprint</a>, we scraped data on shares of special issues and turnaround times. However, our data on the number of published articles and on the number of newly-minted PhDs and world researchers – the core of Frontiers’ rebuke – were downloaded from <strong>publicly available, official data</strong> (via Scimago, OECD, and UNESCO as sources). This is not hidden but plainly stated in our preprint.</p>
<p><strong>Third:</strong> Frontiers writes “<strong>When contacted for access to their dataset</strong>, the authors responded that they had”embargoed” their data, <strong>with the result that no one can verify or replicate their findings</strong>”.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>“When contacted for access to their dataset, the authors responded that they had”embargoed” their data, with the result that no one can verify or replicate their findings.”</em> <br> - Frontiers</p>
</div></div><p>We have norecord of Frontiers requesting our dataset. Our data and scripts are uploaded to FigShare, and <a href="https://figshare.com/articles/dataset/The_strain_on_scientific_publishing_scripts_and_data_/24265726">this data deposition is even visible publicly</a>. While the data are indeed currently under embargo, the fact that they are deposited this way not does not make them unverifiable, but the opposite: it makes them accessible to journal staff and peer reviewers. Thus, this statement by Frontiers is untrue on both counts: we were not contacted for our data, and our results can indeed be replicated and verified, just not publicly.</p>
<p><strong>So why are the data under embargo?</strong> We have consulted with lawyers at our respective institutions throughout this process regarding our data management ethics and responsibilities. Unfortunately, we simply cannot release our web-scraped dataset. The reasons for this are disappointing but sensible: those articles and the metadata contained within belong to the publishers; they are not ours to redistribute. We have a right to scrape and analyse them, but not to share them further without the publishers’ authorisation. Now… if Frontiers authorised us to release the web-scraped data that we collected from them, we could do that immediately.</p>
<p><strong>But!!!</strong> While the raw data are not available, we are free to disclose analyses carried out on them. <strong>In this regard, we have an exciting announcement:</strong> we’ve just released a <a href="https://pagoba.shinyapps.io/strain_explorer/">web app (in beta)</a> that lets you explore data that we can share in a fully-customizable way! See the announcement <a href="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/"><strong>here</strong></a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/strain_explorer.PNG" class="img-fluid"></a></p>
</div></div></section>
<section id="straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said">2. Straw men, or: what we actually said vs.&nbsp;what Frontiers claims we said</h2>
<p>Frontiers claims we said many things we simply never said. We encourage readers to actually read <a href="https://arxiv.org/abs/2309.15884">our manuscript</a> to see what it is we do say. Below we give a brief summary of some of the many straw men populating Frontiers’ piece.</p>
<p><strong>Sampling of Frontiers statements:</strong></p>
<ol type="1">
<li><p>“The study posited that the scientific community is under strain due to a declining workforce and an exponential increase in published articles, caused by special issues in open access journals.”</p></li>
<li><p>“Attributing the growth of scientific output solely to gold open access publishers […].”</p></li>
<li><p>“It is clearly flawed to single out the shift of academic publishing towards open access as the sole driver of increase in scientific output […].”</p></li>
</ol>
<p><strong>Almost all of that is a straw man.</strong>&nbsp;</p>
<ul>
<li><p>We don’t say the workforce is declining, we say (in the Abstract): “Total articles… have grown exponentially… which has outpaced the limited growth – if any – in the number of practising scientists.”</p></li>
<li><p>We never attribute the growth to a single factor – the very aim of our study is to propose <strong>five</strong> indicators of strain covering different factors. Also, (from the discussion): “The strain we characterise is a complicated problem, generated by the interplay of different actors in the publishing market”.</p></li>
<li><p>We never attribute the growth solely to gold OA publishers. Instead, we highlight at least two broad models, stating (in the Discussion): “the amount of strain generated through these two strategies is comparable”; we indicate several problems over our five indicators. <strong>In fact:</strong> we openly and clearly write this (from the Discussion) “regulating behaviours cannot be done at the level of publishing model. Gold open access, for example, does not necessarily add to strain [...].”</p>
<p><strong>These are two things we do say though:</strong></p>
<ul>
<li><p>That growth in yearly published papers is exponential, and</p></li>
<li><p>That the number of researchers has not kept up with that growth</p></li>
</ul>
<p>Frontiers claims these are “false pillars of strain”, suggesting instead that the growth in articles we described is linear, and that the number of scientists over time has seen “a continuous increase.” We will give these suggestions the scrutiny they deserve.</p>
<p><strong>Linearity</strong></p>
<p><strong>First, it bears saying: “Humans tend to systematically underestimate exponential growth and perceive it in linear terms” (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1125810/full">Frontiers in Psychology, 2023</a>).</strong> With this in mind… Frontiers suggested growth in articles was linear by… saying so. While criticizing us for not performing “scientific analysis”, their analysis can be summed up as showing plots and saying “it’s linear!”</p>

<p>However, we ran the stats on this, and over our study’s time period, a linear model is a good fit (R2 = 0.93) but an exponential model is an even better fit (R2 = 0.97, below). Indeed, we reported a mean year-over-year growth of ~5.6% over the period 2016-2022. The bizarre thing is… Frontiers also annotated a 6% year-over-year growth in their Dimensions data from 2017-2022 (above: see the Dimensions data they shared).</p>
<p>Constant absolute growth is linear. Constant year-over-year percent growth is exponential. So in fact, we both agree? The data are exponential. Below we have provided plots of our data with linear or exponential curves annotated, and provided formulae and model fits for both models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="expolinear.png" class="lightbox" data-glightbox="description: .lightbox-desc-4" data-gallery="quarto-lightbox-gallery-4" title="Who wore it better? Linear (red), or exponential (black)?"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/expolinear.png" class="img-fluid figure-img" alt="Who wore it better? Linear (red), or exponential (black)?"></a></p>
<figcaption>Who wore it better? Linear (red), or exponential (black)?</figcaption>
</figure>
</div></li>
</ul>
<div class="no-row-height column-margin column-container"><div class="">
<p><em>“Humans tend to systematically underestimate exponential growth and perceive it in linear terms, which can have severe consequences in a variety of fields.”</em> <br> - Melnik-Leroy, Gerda Ana, et al.&nbsp;“<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1125810/full">Is my visualization better than yours? Analyzing factors modulating exponential growth bias in graphs.</a>” <em>Frontiers in psychology</em> 14 (2023): 1125810.</p>
</div></div><table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 29%">
<col style="width: 10%">
<col style="width: 27%">
<col style="width: 10%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Model</strong></td>
<td><strong>2013-2022</strong></td>
<td><strong>R<sup>2</sup></strong></td>
<td><strong>2000-2022</strong></td>
<td><strong>R<sup>2</sup></strong></td>
</tr>
<tr class="even">
<td>Linear: ax + b</td>
<td>ax = 120379 * yr<br>
b = -240712967</td>
<td>0.93</td>
<td>a = 80319 * yr<br>
b = -159895398</td>
<td>0.96</td>
</tr>
<tr class="odd">
<td>Exponential: a<sup>bx</sup></td>
<td>a = 1.637*106<br>
bx = (5.76* 10-2) (yr - 2013)</td>
<td>0.97</td>
<td>a = 8.74*105<br>
bx = (5.12*10-2) (yr-2000)</td>
<td>0.99</td>
</tr>
</tbody>
</table>
<p><strong>Decline of the number of researchers</strong></p>
<p>Again, we never claimed a decline. From our abstract: “Total articles… have grown exponentially… which has outpaced the limited growth – if any – in the number of practising scientists.”</p>
<p>Some of our plots do show a decline in the number of new PhDs per year. A lower growth rate does not create an absolute reduction – as people all over the planet facing slower inflation this year surely know. Further, we did consider different data types and sources to pin down the growth in the global scientific workforce: some show a plateau, some show only a limited rate of growth. That’s why we said “growth in articles outpaced the limited growth – if any – of researchers”. It’s what the data (from several sources) tell us. Here is our Fig.1supp1, which uses OECD PhD data supplemented with data for India and China (A-B), or UNESCO data on researchers-per-million (C-D):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Fig1supp1_docs_to_researchers.png" class="lightbox" data-glightbox="description: .lightbox-desc-5" data-gallery="quarto-lightbox-gallery-5" title="Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019."><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Fig1supp1_docs_to_researchers.png" class="img-fluid figure-img" alt="Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019."></a></p>
<figcaption>Fig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019.</figcaption>
</figure>
</div>
<p><strong>So, what did we actually say?</strong></p>
<p>We claim, and document, that strain is a real problem, and while it’s not a new problem, it’s become seriously overwhelming in the last few years. We explicitly discuss how <strong>two</strong> main mechanisms have generated this strain: <strong>i)</strong> A steady growth in total journals, and in articles per journal, by legacy publishers like Elsevier and Springer. <strong>ii)</strong> An explosion of articles by publishers adopting special issues as a format to publish the majority of their articles, such as MDPI and Frontiers. In our discussion, we further emphasize these two mechanisms as distinct contributors to strain, and at no point do we suggest that strain is caused by a single publisher or a single publishing behaviour.</p>
<p>Strain is caused by an industry that seeks to grow off the backs of a volunteer workforce that increasingly cannot keep up with demand to write, review, and edit new articles. Read that again and think of your life as a scientist. It resonates, right? What we did is characterize the constitution of that strain, synthesising data that are rarely considered collectively.</p>
<p><strong>Finally, the straw man to bind them all</strong></p>
<p>In their piece, Frontiers makes us out to somehow be “detractors” of the open science movement, setting out to falsely prove our pre-ordained views… while discussing our open access preprint. We are confident our record of public statements and publishing history firmly advocate for open science.</p>
</section>
<section id="of-bad-bibliometrics-that-dont-add-up" class="level2">
<h2 class="anchored" data-anchor-id="of-bad-bibliometrics-that-dont-add-up">3. Of Bad bibliometrics that don’t add up</h2>
<p>We struggled to come up with a delicate way to phrase this, but we simply couldn’t find the words. To be blunt: the Frontiers analysis is amateurish, careless in its data curation and interpretation, leans heavily on visual impressions, and evokes results out of thin air. Let’s dive in.</p>
<p><strong>Frontiers claims:</strong></p>
<p>“The proxy data was <strong>not reproducible</strong> and <strong>not representative</strong> of the original sources of Scopus and Web of Science. The original data does not show the claimed”exponential increase” in the total number of articles published during the study period (2013-2022), <strong>in fact growth is linear</strong> during this period (Figure 2A).”</p>
<p><strong>First:</strong> Frontiers presents this plot of exponential historical article growth, which suggests that the total new articles per year has declined from 2021 to 2022 slightly in Dimensions and Scopus, and by approximately half a million articles per year in the Web of Science. Here is their Figure 1:</p>
<p><a href="Figure_1_Historical_Article_Growth.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Figure_1_Historical_Article_Growth.png" class="img-fluid"></a></p>
<p>However, in their next plot in their Figure 2A (below, left), they instead show a year-over-year increase in the Dimensions and Scopus datasets, and only a minor decline in the Web of Science data. What Frontiers has done in Figure 1 is they have oversmoothed their curve, leading to a faulty data visualization that does not accurately reflect the underlying data.</p>
<p>Their Figure 2A also shows another problem with the analysis carried out by Frontiers, that will occur again later: they claim one thing in the text, provide no numbers in support, and simply reference a figure that, in fact, does not support the claim.</p>
<p>Look at the left panel below, and then the right panel. Frontiers claims that our data are “incomplete and selective” and that “employing reproducible data from original, verifiable sources painted a starkly different picture”. They then provide their Figure 2A, which we give here unaltered in the left panel below. In the right panel, we have cut and pasted the black line (our data) and moved it up so the 2013 data points are aligned between Scopus and “our data”. The growth rate is remarkably similar. This should not surprise anyone, because <strong>our data are Scopus data.</strong> We just filtered them to only include journals also indexed in Web of Science. They are <strong>not</strong> a “proxy, unverifiable source.”</p>
<p><a href="Frontiers_Fig2a.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2a.png" class="img-fluid"></a></p>
<p>But to be sure we do not somehow mislead you with a trick of data visualisation, we crunched the numbers: the correlation between our data and Scopus is: Pearson’s r = 0.992. In fact, the correlation between our data and the Dimensions data Frontiers provided us is: Pearson’s r = 0.983. So even if you use Dimensions or Scopus instead, <strong>our data do paint the same picture.</strong></p>
<p><strong>Pillars made of soapstone:</strong> another example of claiming a result, not running any analysis, and then reaching a conclusion not supported by the figure is their “second pillar of strain”: the limited growth – if any – in the number of researchers. Here is Frontiers’ claim:</p>
<p>“The study uses PhD graduates as a proxy for active researchers who write, review, and edit articles when a more direct measure of active researchers is available… They ignore the upward trend available from the same data source they used (Figure 2B) and fail to mention or consider that PhD graduations were disrupted during the pandemic. A direct measure of the number of active researchers shows the opposite to what they claim – a continuous increase.”</p>
<p>Again, we never say there has been a decrease in researchers (see above). Frontiers then provides their Figure 2B and claims it shows a continuous increase in researchers.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><a href="Frontiers_Fig2b.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2b.png" class="column img-fluid"></a></p>
</div><div class="column" style="width:5%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:55%;">
<p>But we tend to see a plateau in this plot, similar to the plateau we saw in newly-minted PhDs. And again, the reduction in newly-minted PhDs was robust to inclusion of data from India and China (see Fig.1supp1 above). We further double checked this trend using UNESCO data on researchers per million, similar to what Frontiers has done here, finding a very limited growth in recent years. That’s why we only claimed that article growth had outpaced the growth - if any - in active researchers.</p>
</div>
</div>
<p><strong>There are actually additional problems with the data that Frontiers plot here:</strong> the long timeframe they show is misleading because the countries indexed in the OECD dataset have evolved over time. Indeed, the huge spike in 2013 is due to the inclusion of many new countries, including Germany, explaining this large spike in total new researchers.</p>
<p>Further, Frontiers claims the COVID-19 pandemic is behind the plateauing of PhD graduates that we saw. However, this plateau, also seen in the total researchers data above, started in ~2017. So we can’t exactly attribute this to the pandemic. So again, none of what Frontiers claims is true: there has not been a continuous increase in total researchers, but rather the same plateau we saw in PhD graduate numbers - the agreement between these two metrics is also quite sensible, considering PhD graduates become researchers.</p>
<p><strong>Frontiers actually validates our data:</strong> this response would not be complete without a look at Figure 2C. It purports to show the absence of correlation between total articles per year being generated and the number of Frontiers articles published in “Research Topics” (i.e.&nbsp;special issues). Frontiers plots the datasets of Scopus and Web of Science as lines, and then its own Research Topic articles as very small bars, choosing two different visualization styles within the same plot.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><a href="Frontiers_Fig2c.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/Frontiers_Fig2c.png" class="column img-fluid"></a></p>
</div><div class="column" style="width:5%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:55%;">
<p><strong>Briefly:</strong> there is actually quite a good correlation between total articles and Frontiers Research Topic articles per year. The correlation between the Scopus data in green and the Frontiers Research Topic data is: Pearson’s r = 0.912. Moreover, the growth in Frontiers Research Topic articles has been exponential: a mean year-over-year growth of 43%, far outpacing the overall strain (~6%).</p>
<p><strong>But somewhat ironically:</strong> this plot actually validates our own scraped data from Frontiers. As we mentioned, we requested data from Frontiers on special issue article numbers so we could validate or even replace our web-scraped data, but they never sent us those data. <strong>But in this Figure 2C, in fact, Frontiers provides the data we requested.</strong> So, how did we do? It seems we were spot-on.</p>
</div>
</div>
<p>Our web-scraped dataset estimated 69.3% of articles in Frontiers were published through “Research Topics” (i.e.&nbsp;special issues). Applying this estimate to their 2022 output (<a href="https://progressreport.frontiersin.org/">125.1k articles</a>) yields ~87k special issue articles in 2022 – the Figure here says 88k. Thus, <strong>our scrape of Frontiers articles yielded ~99% accuracy</strong> – pretty impressive given we only scraped the 49 journals from our study, while the 88k reflects all 200+ Frontiers journals. Thank you to Frontiers for providing these data that validates our web-scraping.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We started a conversation with Frontiers while we were working on our article. We offered them the chance to comment on our work before we released it publicly. This is a courtesy we also extended to other publishers. We were thus surprised to find this blog posted without our knowledge. Indeed, we had numerous emails with Frontiers in the lead-up to our work that were good-faith exchanges. We even have the original comments that Frontiers provided to us from when we sent them our draft article: they praised aspects of our work and provided constructive feedback, including correcting our phrasing to avoid ambiguous wording. We would be happy to share those comments, if Frontiers would give us permission to. We still thank Frontiers for those comments.&nbsp;</p>
<p>That is why it is so surprising to see this latest piece, which contains a startling level of animosity and many derogatory accusations that are simply untrue.</p>
<p>Others, including publishers, have welcomed our scientific countribution. Frontiers could have done the same. Instead, Frontiers has released this blog containing many analytical errors just one week after they published <a href="https://www.swissinfo.ch/eng/science/wrong-ai-generated-images-in-scientific-journal-put-a-strain-on-swiss-publisher-frontiers/73657004">the questionable article on rodent genitalia</a>. Of course mistakes happen, and Frontiers retracted that article swiftly. But this? This is something else. These are their own words, chosen intentionally.&nbsp;</p>
<p>We still want to thank specific employees at Frontiers with whom we had a respectful relationship. We made every effort we could to adhere to ethical research conduct. We had good faith, mutually beneficial exchanges with Frontiers. Thus why it is so disheartening to see this blog post and the tone it took. We hope Frontiers will reflect on what they’ve said, how they’ve said it, and choose to engage with us more productively in the future.</p>
<p>We wrote the Strain paper because we wanted there to be more transparency over the data that publishers control on academic publishing; because we thought we needed a more strongly data-driven conversation about publishing trends; because some of the contributions to that conversation are misguided, dysfunctional and require rectifying, and because we think researchers need to scrutinise publishers’ data if they are to be effectively analysed and interpreted. We cannot assume publishers will do that well.</p>
<p>The Frontier’s blog confirms us in that view.</p>
<p><em>Sincerely,</em></p>
<p><em>Mark A. Hanson, Pablo Gómez Barreiro, Paolo Crosetto, Dan Brockington</em></p>



</section>

 ]]></description>
  <category>Frontiers</category>
  <category>The Strain</category>
  <category>Open Access</category>
  <category>Scientific Publishing</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/</guid>
  <pubDate>Wed, 13 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/response_to_frontiers/thumbn2.png" medium="image" type="image/png" height="112" width="144"/>
</item>
<item>
  <title>Announcement: “The Strain on Scientific Publishing” data explorer!</title>
  <dc:creator>Pablo Gómez Barreiro</dc:creator>
  <dc:creator>Mark Hanson</dc:creator>
  <dc:creator>Paolo Crosetto</dc:creator>
  <dc:creator>Dan Brockington</dc:creator>
  <link>https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/thumbn.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="627" alt="The Strain Explorer app is here!"></a></p>
</figure>
</div>
<figcaption>The Strain Explorer app is here!</figcaption>
</figure>
</div>
<p>A common sentiment we have received is that it is a shame we cannot release our data. We think so too. But the legal advice about how to treat web-scraped data is clear: we cannot share the raw data<sup>1</sup>.</p>
<p>But… After many consultations, we’ve figured out exactly what data we <strong>can</strong> share, and so we’re doing just that. We’ve provided the first of some step-by-step guides for you to download the raw data for yourself (complete with screenshots!), and R code to assemble Scimago yearly data into a single dataframe. Take a look <strong>here</strong></p>
<p>But if that sounds like a lot of work, <strong>we’re building an R-based shiny app</strong> that lets you type in your journal or publisher of interest and get reports on strain added by each publisher, proportion of special issue articles, average turnaround times per journal, and impact inflation!</p>
<p>We were planning to share this during our revisions, but we’ve decided to release a sneak peek: <a href="https://pagoba.shinyapps.io/strain_explorer/"><strong>we now have a beta version of this “Strain data explorer” on our website</strong></a><strong>.</strong> As this app is in beta, we expect a few bugs. We’ve also got some data visualization ideas to develop, and some kinks to fix. We’ll be making improvements over time, so do check back for updates, and do send in improvements and suggestions on what you’d like to see it show!</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="https://pagoba.shinyapps.io/strain_explorer/"><img src="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/strain_explorer.PNG" class="img-fluid"></a></p>
</div></div><p>Cheers,</p>
<p>Pablo Gómez Barreiro, Mark A. Hanson, Paolo Crosetto, and Dan Brockington</p>
<div id="callout-1" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The nitty gritty details
</div>
</div>
<div class="callout-body-container callout-body">
<p>For those interested in accessing raw data, here are technical notes and instructions on how you can assemble those data for yourself.</p>
<p><strong>Scripts</strong></p>
<p>In the file Shiny_scripts_data_release.zip there is a folder setup in an R package format. Download <a href="https://github.com/the-strain-on-scientific-publishing/website/blob/main/data/Shiny_scripts_data_release.zip">this .zip file from Github</a>, unzip the file, and open the Rproject and the main analysis script. The “Scripts” folder does not need to be touched. But you will need to follow the instructions below to populate the folders with the data needed for running the analysis.</p>
<p><strong>Data related to Figures 1 &amp; 5</strong></p>
<p>These data come from Scimago, OECD, and UNESCO. This .pdf contains step-by-step instructions on how to assemble these data for yourself.</p>
<p><strong>Data related to Figures 2, 3, and 4</strong></p>
<p>Unfortunately, these data we simply cannot share. If publishers authorize us to release the data we have on them, we could. But until then, we have to keep these data private.</p>
<p>Still, we can explain how we collected these data, and if you’re coding-savvy, you can follow along. Pablo has been writing blog posts [<a href="https://pgomba.github.io/pgb_website/posts/08_10_23/">1</a>, <a href="https://pgomba.github.io/pgb_website/posts/11_10_23/">2</a>, <a href="https://pgomba.github.io/pgb_website/posts/18_10_23/">3</a>] on how he scraped different publishers, so stay tuned for updates. The current posts already guide you through the process for example publishers. In brief: we had to take a unique strategy to scraping each publisher, based on how they built their web HTML code. This is why our study is more limited in which publishers we analyse in Figures 2, 3, and 4. But the idea is generally the same for each publisher:</p>
<ul>
<li>Feed the script a list of DOIs/addresses</li>
<li>Search the web page for lines in the html that contain the data of interest.</li>
<li>For instance, the line below can be gives the date the article was put online: “<em>meta name=”citation_online_date” content=”2018/01/23”</em>”</li>
<li>Customize the script for each data point you want to collect, for each publisher, sometimes in subgroup-specific ways…</li>
<li>Check your data very very carefully for errors. Web pages can themselves contain errors even if your script is good! For example, we got quite a few articles published online Jan 1st 1970 (the so-called “epoch date”).</li>
</ul>
<p><strong>A technical note on Impact Inflation</strong></p>
<p>In the preprint we used a 2-year window for Clarivate Impact Factor or Scopus cites/doc. However the standard calculation of Scimago Journal Rank (SJR) uses a 3-year window. As a result, changes in SJR lag changes in Impact Factor a bit. Because these are values derived from 2-3 years of citation behaviour, no single year totally throws off the calculation, but this difference in time window can introduce noise unnecessarily. We prefer to use 2-year Impact Factors and cites/doc values in the preprint as this is the most common form of Impact Factor, and so more intuitive for the reader.</p>
<p>But we can avoid this lag effect if we simply use 3-year windows for Impact Factors or cites/doc. So in the shiny app, we have used Scimago cies/doc (3-years) / SJR to calculate Impact Inflation. These numbers are very similar to the values you get from using a 2-year window for cites/doc, but for journals with big changes to cites/doc over time, the result is a slightly lower volatility in Impact Inflation from year-to-year as a result.</p>
</div>
</div>




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Researchers have a right to web-scrape information and to use it for scientific analyses, but not to further release the raw data scraped. Those data belong to the web-scraped parties (in this case, publishers), who retain rights over their distribution.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Data explorer</category>
  <category>Shiny</category>
  <category>The Strain</category>
  <category>Scientific Publishing</category>
  <guid>https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/</guid>
  <pubDate>Wed, 13 Mar 2024 00:00:00 GMT</pubDate>
  <media:content url="https://the-strain-on-scientific-publishing.github.io/website/posts/app_announcement/thumbn.png" medium="image" type="image/png" height="81" width="144"/>
</item>
</channel>
</rss>
