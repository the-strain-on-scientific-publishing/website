[
  {
    "objectID": "posts/app_announcement/index.html",
    "href": "posts/app_announcement/index.html",
    "title": "Announcement: “The Strain on Scientific Publishing” data explorer!",
    "section": "",
    "text": "The Strain Explorer app is here!\nA common sentiment we have received is that it is a shame we cannot release our data. We think so too. But the legal advice about how to treat web-scraped data is clear: we cannot share the raw data1.\nBut… After many consultations, we’ve figured out exactly what data we can share, and so we’re doing just that. We’ve provided the first of some step-by-step guides for you to download the raw data for yourself (complete with screenshots!), and R code to assemble Scimago yearly data into a single dataframe. Take a look here\nBut if that sounds like a lot of work, we’re building an R-based shiny app that lets you type in your journal or publisher of interest and get reports on strain added by each publisher, proportion of special issue articles, average turnaround times per journal, and impact inflation!\nWe were planning to share this during our revisions, but we’ve decided to release a sneak peek: we now have a beta version of this “Strain data explorer” on our website. As this app is in beta, we expect a few bugs. We’ve also got some data visualization ideas to develop, and some kinks to fix. We’ll be making improvements over time, so do check back for updates, and do send in improvements and suggestions on what you’d like to see it show!\nCheers,\nPablo Gómez Barreiro, Mark A. Hanson, Paolo Crosetto, and Dan Brockington"
  },
  {
    "objectID": "posts/app_announcement/index.html#footnotes",
    "href": "posts/app_announcement/index.html#footnotes",
    "title": "Announcement: “The Strain on Scientific Publishing” data explorer!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nResearchers have a right to web-scrape information and to use it for scientific analyses, but not to further release the raw data scraped. Those data belong to the web-scraped parties (in this case, publishers), who retain rights over their distribution.↩︎"
  },
  {
    "objectID": "posts/mdpi_scr_comment/index.html",
    "href": "posts/mdpi_scr_comment/index.html",
    "title": "How much does a journal weight? A commentary on MDPI’s own study on their self-citations rates",
    "section": "",
    "text": "A recent blog published by the Association of Learned and Professional Society Publishers, written by MDPI staff Dr. Giulia Stefenelli and Dr. Enric Sayas, explored MDPI and other publishers self-citations in 2024. In line with MDPI usual transparency, they kindly included the data they used, along with the relevant code in Python.\nFigure 1 in their blog instantly caught my attention, and my commentary on their blog is mainly around this figure and its interpretation.\n\n\n\nFigure 1, as seen in original blog. Left y-axis represents total documents per publishers in year 2024 (blue columns), while right axis shows average self-citation per publishers in 2024 (orange dots).\n\n\nFigure 1 is easily reproducible thanks to the provided (and well-documented) script, top_10.py. Now, I can do a little bit of Python, but I’m less likely to make mistakes in my native language: R. I’ve teamed up with chatGPT to translate top_10.py into top_10_PGB.R, and the code necessary to replicate this commentary is available here: [Github link]. The conversion outputs same data as their Table 1 and similar graph (I took the liberty of some aesthetic changes):\n\n\n\nMy attempt to replicated original graph. Good enough?\n\n\nHaving Total Documents in this graph was masking the information conveyed by Average Self-citation rates. Here is the data for Average self citation rates with publishers rearranged by it.\n\n\n\nGraph showing average self-citation values (2024) from Blog’s Figure 1 by publishers arranged by rate valueverage self-citing rate values.\n\n\n\n\n\n\n\nQuote from MDPI’s self assessment on self-citation rates\n\n\nAs discussed on the blog, MDPI ranks 6th in self-citation among the largest publishers. However, there is a major issue with how this data has been analyzed: the average self-citation rate was calculated by simply averaging each journal’s self-citation rate without accounting for the total number of publications per journal. In other words, every journal contributed equally to the average, regardless of its size.\nHere, I present the results of the analysis when the means are weighted by the total number of documents published per journal in 2024:\n\n\n\nGraph showing average self-citation rates (2024) after being corrected via weighted means by total number of publications by journal.\n\n\nAdditionally, here is a summary table comparing self-citation rates before and after considering weighted means.\n\n\n\n\n\n\n\nTable showing original and corrected (weighted) self-citation rates (2024) in %, along with difference between these values\n\n\nOverall, MDPI is the most affected publisher after applying weighted means. The reanalysis of the data using weighted means moves MDPI from 6th position (with a 14% self-citation rate) to 3rd position (with a 19.7% self-citation rate). Notably, the previous table leaders, OUP and T&F, remain in their respective positions with little change in their final percentages, likely due to the balance of total documents across their journals. This contrasts with the higher threshold of total documents per journal in MDPI, possibly driven by larger journals having higher levels of self citation than smaller ones. Lets find out:\n\n\n\n\n\n\n\nMDPI individual journals plotted by total number of articles published in 2024 (x-axis) and self-citing rate % (y-axis\n\n\nThe data shows that, out of the 237 selected journals, 57 have a self-citation rate over 20%, although only 25 have more than 1,000 articles published in 2024. MDPI published 193,873 articles, of which 47% where published in journals with a self-citation rate over 20%. This % decreases down to 10.8% for number of articles published in journals with self-cite rates over 30%. Assigning each article the journal’s self-citation rate provides an alternative perspective on MDPI’s decision to plot journal frequency by average self-citation rate.\n\n\n\nA different take on self-citations. Plotting frequency of articles instead of average journal self-citation\n\n\n\n\n\n\n\nOriginal histogram with average journal self-citation\n\n\nWeighted means present a different perspective on the 2024 self-citation landscape, making it important to analyze them in a multi-publisher context. However, conclusions drawn from both the original and reinterpreted graphs still come with significant caveats:\n\n\n\n\n\n2018-2021 MDPI self-citation rates from previous publication\n\n\n\nThe time window is limited to 2024. Temporal context is crucial, especially for understanding shifts in self-citation trends in modern publishing. A previous MDPI self-assessment on self-citations, covering the period from 2018 to 2021, showed self-citation rates close to 30%.\nPublishers have different balances of natural sciences and humanities in their coverage, and each discipline may exhibit varying self-citation rates.\n\nIn conclusion, analyzing self-citation at the publisher level requires the use of weighted data to be truly effective, especially to avoid biases introduced by the disparity in journal sizes. I encourage MDPI (and other publishers) to try this approach in further self-analysis of their practices."
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "The Daily Strain",
    "section": "",
    "text": "How much does a journal weight? A commentary on MDPI’s own study on their self-citations rates\n\n\n\nR\n\nMDPI\n\nself-citations\n\n\n\n\n\n\n\n\n\nMar 23, 2025\n\n\nPablo Gómez Barreiro\n\n\n\n\n\n\n\n\n\n\n\n\nResponse to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”\n\n\n\nFrontiers\n\nThe Strain\n\nOpen Access\n\nScientific Publishing\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nMark Hanson, Pablo Gómez Barreiro, Paolo Crosetto, Dan Brockington\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncement: “The Strain on Scientific Publishing” data explorer!\n\n\n\nData explorer\n\nShiny\n\nThe Strain\n\nScientific Publishing\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nPablo Gómez Barreiro, Mark Hanson, Paolo Crosetto, Dan Brockington\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "The Daily Strain"
    ]
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media related to our project",
    "section": "",
    "text": "Radio & Podcasts\n\nMark was interviewed by Jonny Coates for the Preprints in Motion podcast\nMark, Pablo and Paolo were interviewed on Episode 7 of the Science of doing Science podcast held by the University of Cyprus SinnoPSis team\nMark participated to the Tech & Science Daily podcast run by the Evening Standard. Listen to Mark here!\nPablo was interviewed live on Radio 5 [Spanish]. Listen here: Ciencia con pies de barro\nPablo was interviewed by Cadena Ser [Catalan/Spanish]. Download section here (Github link)\nPablo participated in the “Fundación Margarita Salas” podcast discussing the scientific publishing current panorama: Link\n\n\n\nVideos\n\nPaolo made a video to summarise the study for the Open Science directorate of INRAE [French]. Here: L’édition Scientifique sous pression - décryptage par paolo Crosetto\nDan and Mark presented at the 384th CGHE webinar, and there is a Youtube video of the seminar.\n\n\n\n\nDan presented our work in Tanzania, including an interview in Swahili here. The full video of the nearly 2-hour long event, in English with some Swahili, is here on Youtube (and below)\n\n\n\n\nDan interviewed by the Tanzanian channel Mlimani TV [in Swahili]\n\n\n\n\nPaolo presented our work in Italian in Milan, at an Open Science conference held at the headquarters of UniMi on September 13th, 2024. The webpage of the event is here; and here you find the full video of the conference, including introduction by the Dean of the University of Milan Elio Franzini and a talk by philosopher Haikel Hosni.\n\n\n\nOthers\nViaje al lado oscuro de la ciencia [Spanish] - Building on “The Strain” paper, this is a brief comment on all things wrong affecting the current scientific publishing panorama. Published in Malaga´s University magazine, pArAdigmA (volúmen 26, 2024).",
    "crumbs": [
      "Videos & audios"
    ]
  },
  {
    "objectID": "delete.html",
    "href": "delete.html",
    "title": "Untitled",
    "section": "",
    "text": "Tip 1: Cross-Referencing a Tip\n\n\n\nAdd an ID starting with #tip- to reference a tip.\n\n\nSee Tip 1…"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the authors",
    "section": "",
    "text": "Biologist interested in immunity and host pathogen interactions @ University of Exeter, UK, using Drosophila\n\nInstitutional website\nBlog and personal website\nGoogle scholar\nBluesky\nMastodon",
    "crumbs": [
      "About the authors"
    ]
  },
  {
    "objectID": "about.html#mark-a-hanson",
    "href": "about.html#mark-a-hanson",
    "title": "About the authors",
    "section": "",
    "text": "Biologist interested in immunity and host pathogen interactions @ University of Exeter, UK, using Drosophila\n\nInstitutional website\nBlog and personal website\nGoogle scholar\nBluesky\nMastodon",
    "crumbs": [
      "About the authors"
    ]
  },
  {
    "objectID": "about.html#pablo-gómez-barreiro",
    "href": "about.html#pablo-gómez-barreiro",
    "title": "About the authors",
    "section": "Pablo Gómez Barreiro",
    "text": "Pablo Gómez Barreiro\n\n\nAgricultural Engineer with a Plant Breeding MSc and +10 years of experience in the Plant & Seed Science sector. Currently working as a Seed Biology Lab. Technician at the Millennium Seed Bank (Royal Botanic Gardens, Kew)\n\nInstitutional website\nPersonal website\nGoogle Scholar\nBluesky",
    "crumbs": [
      "About the authors"
    ]
  },
  {
    "objectID": "about.html#paolo-crosetto",
    "href": "about.html#paolo-crosetto",
    "title": "About the authors",
    "section": "Paolo Crosetto",
    "text": "Paolo Crosetto\n\n\n\n\n\n\n\n\n\n\nExperimental economist • Senior Researcher @ INRAE, Grenoble, France • Food labeling • Risk • Open licenses • R enthusiast\n\nInstitutional website\nBlog and personal website\nGoogle Scholar\nTwitter\nMastodon",
    "crumbs": [
      "About the authors"
    ]
  },
  {
    "objectID": "about.html#dan-brockington",
    "href": "about.html#dan-brockington",
    "title": "About the authors",
    "section": "Dan Brockington",
    "text": "Dan Brockington\n\n\nICREA Research Professor at the Universitat Autonoma de Barcelona, Institut de Ciència i Tecnologia Ambientals (ICTA)\n\nInstitutional website\nBlog and personal website\nGoogle Scholar\nTwitter",
    "crumbs": [
      "About the authors"
    ]
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Presentations",
    "section": "",
    "text": "Presentations\n\nPreprint/Published paper\n\nSeptember 2024, Paolo presented our results (in Italian) at an open science event in Milan, at the presence of the Dean of the University fo Milan. Slides in Italian here.\nMay 2024, Paolo presented our results in a brief intervention at the Global Research Council meeting in Interlaken, Switzerland. Slides in English here.\nSpring/Summer 2024 Paolo gave several presentations based on an updated set of slides (in English): here the slides for the NEOMA Workshop on June 19th, but similar slides were used for other talks (Basel, Israel,\nMarch 2024, Paolo presented (in French) at the Université de Rennes, IRMAR ethics seminar – slides in French here.\nMarch 2024, Paolo presented in Paris at the Open Science day of the Graduate School University Paris-Saclay. Here the slides.\nDecember 2023, Dan & Mark presented at the 385th CGHE webinar. Here the slides. You can also visit the website of the event and a youtube video of the seminar.\nNovember 2023, Semaine de la Science Ouverte de l’Université de Bourgogne Franche Comté (in French) by Paolo. Here the slides.\n\n\n\nPreliminary\n\nMay 2023, University of Exeter (with preliminary results) by Mark. Here the slides.\nApril 2023, SNSF Swiss National Science Foundation (with preliminary results) by Paolo. Here the slides.",
    "crumbs": [
      "Slide decks"
    ]
  },
  {
    "objectID": "in-the-press.html",
    "href": "in-the-press.html",
    "title": "Articles, threads, posts… about our paper",
    "section": "",
    "text": "Altmetric\n\n\n\n\n  \n   \n  \n\n\n\nTwitter + BlueSky + Mastodon threads\n\nPreprint\n\nMastodon thread by Mark A Hanson\nTwitter thread by Paolo Crosetto\nTwitter thread by Dan Brockington\nTwitter thread by Matthias Egger\nTwitter thread by Andrew Akbashev\nBlueSky thread by Marc Veldohen\n\n\n\nPublished version\n\nBlueSky thread by Mark\nTwitter thread by Paolo\n\n\n\n\nPress\n\nEnglish\n\nTimes Higher Education [EN] - Interviewed Mark and cited Paolo’s earlier work on the decision of JUFO to downgrade most MDPI and Frontiers titles.\nThe Economist – England [EN] – Scientific publishers are producing more papers than ever\nPress release - University of Exeter\nScience [EN] - Scienceadviser 16 november 2023\nEl País [EN] - Public funds being swallowed up by scientific journals with dubious articles\nFinancial Times [EN]- How academic publishers profit from the publish-or-perish culture\nNewswise [EN] - Avalanche of papers could erode trust in science\nPhys.org [EN] - Avalanche of published academic articles could erode trust in science\nIndia Education Diary [EN] - The Potential Avalanche of Papers: A Concern for Trust in Science\nPaudal [EN] - The “avalanche” of articles can undermine trust in science, warns a study\nWall Street Journal [EN] - What’s Wrong With Peer Review?\nWall Street Journal [EN] - Retractions, Walkouts Plague Science – Journals Eager to Churn Out Research\nBrisbane Times [EN] - We’ve hit peak science, and that’s not good\nSidney Morning Herlad [EN] - We’ve hit peak science, and that’s not good\nChemical & Engineering News [EN] - Is the academic social networking site ResearchGate still relevant?\nThe Citizen – Tanzania [EN] - Mushrooming academic papers: Researchers’ concern about scientific trust\nSwissinfo.ch – Switzerland [EN] - AI-generated rat genitalia: Swiss publisher of scientific journal under pressure\n\n\n\nFrench\n\nLe Monde [FR] article by David Laroussière\nActualité web [FR] on the INRAE website\nL’Express [FR] - article by Antoine Beau on the Hindawi crisis and shutdown\nL’Express [FR] - long article by Antoine Beau featuring an interview to Paolo on the “Bubble” of scientific publishing.\n\n\n\nSpanish\n\nPress release [ES] - Universitat Autònoma de Barcelona\nEl País [ES] - La burbuja de las revistas científicas se traga millones de euros de dinero público\nEl País [ES] - Virus en la publicación científica\nLa Vanguardia [ES] | Agencia EFE [ES] | Infobae [ES] | Yahoo [ES] | QuéPasa [ES] | MSN|[ES]News ES Euro [ES] | HM TV [ES] | BellerDigital [ES] | Copiroyal [ES] | Minuto 30 [ES] | Portal Político TV [ES] | Canalpaís américa [ES] | El Periódico de México [ES] | CubaSí [ES] | El Nacional [ES] | El Diario | ABC Paraguay [ES] | Listin Diario [ES]: La “avalancha” de artículos puede socavar la confianza en la ciencia, alerta un estudio\n\n\n\nCatalan\n\n3CAT [Cat] - Alerten d’una “allau” d’articles científics que es publiquen sense revisar-ne la qualitat\n\n\n\nPortuguese\n\nOutras Palavras [PT] - Os mercadores globais do saber\nJournal da Unesp [PT] - Produtivismo e imediatismo da Academia confundem o público\n\n\n\nDutch\n\nNRC [NL] - 2.800.000\n\n\n\nGerman\n\nLaborJournal [DE] - Sonderausgaben – Fluch oder Segen?\n\n\n\nArabic\n\nHespress [Morocco]\n\n\n\nRomanian\n\nHotNews.ro [RO]\n\n\n\nNorwegian\n\nForskning.no [NO] - also translated in English in their sciencenorway English-language website\n\n\n\nSwahili\n\nLango La Habar [Tanzania]\n\n\n\nVietnamese\n\nThanh Niên [Vietnam]\n\n\n\nIndonesian\n\nThe Conversation [Indonesia]\n\n\n\n\nBlogs\n\nIn the Dark [EN] – the blog of astrophysicits Peter Coles\nBishop Blog [EN] - the blog of Dorothy Bishop. Also:\n\nhere when discussing her brilliant Defense Against the Dark Art course proposal\nhere when discussing issues with a series of papers at MDPI\n\nRevues et Integrité [FR] – the blog of Hervé Maisonneuve – 2023\nRevues et Integrité [FR] – the blog of Hervé Maisonneuve – 2024\nThe Blot Report [EN]\nWeekend list of Retraction Watch [EN] - 7/10/2023\nDigital Koans [EN] - Blog of Digital Scholarship\nLSE Impact blog [EN] – a Social Science impact Blog from the London School of Economics\nScholarly Kitchen [EN] - an article about consolidation in the publishing market cites us for reference\nScholarly Kitchen [EN] - an article about he rise of AI and the future of scientific publications\nThe Brief [EN] - monthly newsletter about scientific publishing by Clarke & Esposito talks about our paper in their “briefly noted” section\nReasonable people [EN] - the substack blog of Tom Stafford\nNanoscale views [EN] - the physics blog of Douglas Natelson\nGiuseppe Vizzari’s blog [EN] - Contributing to the public goods game…\n/USR/SPACE [EN] - Michael Rowe’s blog\nDaily Nous [EN] - Philosophers, Should You Pay to Publish Your Paper? by Justin Weinberg\nFrontiers’ Science News [EN] - Bad bibliometrics don’t add up for research or why research publishing policy needs sound science. This is a very critical piece. We explain why it’s incorrect and faulty in our reply here.\nLSE Impact Blog [EN] - Academia can no longer ignore its systemic inter-generational inequality\nFederation of European Biochemical Societies Network [EN] - Beyond the journal: The future of scientific publishing\nInternational Science Council [EN] - More is not better: the developing crisis of scientific publishing\nAfrica at LSE [EN] - We need to put Open Access journals at the heart of academic publishing\nMesures et démesure de la publication scientifique [FR]\nDesafíos en la Publicación Científica: El Impacto del Crecimiento Exponencial en la Rigurosidad y Calidad [ES] - Blog on Peru newspaper Gestión\nCrooked Timber [EN] – For-Profit Academic Publishers Love LLM Garbage by Keving Munger\nLooking at nothing – Fixing science: a new approach for trust? [EN]\nQuo vadis, Open Access [SI] – from Slovenian media network Metina Lista (Mint leaf)\nThe oligopoly publishers [EN] – from David Rosenthal’s blog\n\n\n\nOnline magazines\n\nROARS [IT] - the Returns on Academic Research and School website interviewed Paolo on the preprint.\nThe Meta News [FR] - access restricted to researchers in French universities and Research Institutes\nSciELO en perspectiva [PT] - Portuguese blog hosting an article by Lilan Nassi-Calo\nUpstrean [EN] - Drinking from the Firehose? Write More and Publish Less",
    "crumbs": [
      "Mentions & articles"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Strain on Scientific Publishing",
    "section": "",
    "text": "This web page hosts the paper “The Strain on Scientific Publishing” by Mark A Hanson, Pablo Gómez Barreiro, Paolo Crosetto and Dan Brockington. It is a repository for the paper and all activity revolving around it – blog posts, news coverage, audio, video, presentations, comments and criticism.\n\nThis website will also host, when this will be possible, the data underlying the paper, as well as the R code used to generate our analysis and allowing anyone to reproduce it. This is unfortunately not yet possible for legal reasons (as of October 2023).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#the-paper",
    "href": "index.html#the-paper",
    "title": "The Strain on Scientific Publishing",
    "section": "The paper",
    "text": "The paper\nYou can find the published paper on Quantitative Science Studies.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "posts/response_to_frontiers/index.html",
    "href": "posts/response_to_frontiers/index.html",
    "title": "Response to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”",
    "section": "",
    "text": "Illustrated by Jasmina El Bouamraoui and Karabo Poppy Moletsane, distributed under CC0 1.0\nWe have received a diverse, global, and humbling response to our preprint “The Strain on Scientific Publishing.” We thank everyone for the kind words, thoughtful commentaries, and critiques from across the spectrum. We embrace the opportunity to critically re-evaluate our positions and better our understanding of the data. This is Open Science at its best, and we fully support it.\nA recently-published Frontiers Media “science news” blog commented publicly on our work. We read this piece with great interest, but were surprised to see this blog mischaracterize our work and the relationship we had with Frontiers in the lead-up to releasing our preprint. We feel the debate around our preprint is best left to the scientific discourse, including peer review; ultimately the worth of our work to the scientific community at large will be judged by the utility that it provides to understanding the academic publishing landscape.\nHowever, we feel compelled to publicly reply to this blog piece as it contains factual errors, distorts our work, accuses us of editing, cutting and omitting data to produce biased results. These accusations are particularly hideous to make of scientists. Frontiers’ blog further produces an alternative analysis that partly does not stand up to scrutiny, and partly supports the very results it claims to debunk.\nHere we provide a response focused on the most salient points. We must reply to the claims made about our character seriously: we did not cut, distort, omit or edit any data. Here, we will set the record straight and dispel the derogatory accusations contained in Frontiers’ piece. Without further ado:"
  },
  {
    "objectID": "posts/response_to_frontiers/index.html#frontiers-data-claims",
    "href": "posts/response_to_frontiers/index.html#frontiers-data-claims",
    "title": "Response to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”",
    "section": "1. Frontiers data claims",
    "text": "1. Frontiers data claims\nFirst: we were surprised to see Frontiers claiming they had shared data with us. We asked them for data on several occasions between April-September 2023, and while they always responded with “we’re working on it,” they never provided us any of the data we requested on special issue articles, turnaround times, or rejection rates. So to be clear: we requested data from Frontiers such that we could validate our web scraping, but they never shared those data with us.\n\n\n“In fact, using the same data Frontiers provided the preprint authors as well as the correct original Scopus data, we do not find any correlation between total number of articles and the number of special issues articles (Figure 2C).”  - Frontiers\nThey did send us a personally-curated set of Dimensions data in August 2023. They did so after we shared a draft of our preprint prior to its release. Here’s a screenshot of the full extent of the data Frontiers sent us:\n\n\n\nCaption from data shared by Frontiers. Sensible information redacted\n\n\nWe have redacted rows in this spreadsheet that focused on another publisher. We didn’t ask Frontiers for Dimensions data, and we are also quite capable of curating data for ourselves. We opted to use a conservative dataset where all journals in our analysis were indexed by both Scopus and Web of Science (downloaded from Scimago), such that the growth in articles we described would relate solely to journals indexed by both industry heavyweights. We acknowledge in our study that there are even more journals out there, for instance, journals indexed by Dimensions but not by both Scopus and Web of Science, and so the strain we describe is likely an underestimate.\nSecond: Frontiers writes that we “resorted to unverifiable data obtained through web scraping”.\n\n\n“The authors resorted to unverifiable data obtained through web scraping, which means that the study and its conclusions were predicated on an incomplete and unbalanced dataset of publisher activity.”  - Frontiers\nFor our preprint, we scraped data on shares of special issues and turnaround times. However, our data on the number of published articles and on the number of newly-minted PhDs and world researchers – the core of Frontiers’ rebuke – were downloaded from publicly available, official data (via Scimago, OECD, and UNESCO as sources). This is not hidden but plainly stated in our preprint.\nThird: Frontiers writes “When contacted for access to their dataset, the authors responded that they had”embargoed” their data, with the result that no one can verify or replicate their findings”.\n\n\n“When contacted for access to their dataset, the authors responded that they had”embargoed” their data, with the result that no one can verify or replicate their findings.”  - Frontiers\nWe have norecord of Frontiers requesting our dataset. Our data and scripts are uploaded to FigShare, and this data deposition is even visible publicly. While the data are indeed currently under embargo, the fact that they are deposited this way not does not make them unverifiable, but the opposite: it makes them accessible to journal staff and peer reviewers. Thus, this statement by Frontiers is untrue on both counts: we were not contacted for our data, and our results can indeed be replicated and verified, just not publicly.\nSo why are the data under embargo? We have consulted with lawyers at our respective institutions throughout this process regarding our data management ethics and responsibilities. Unfortunately, we simply cannot release our web-scraped dataset. The reasons for this are disappointing but sensible: those articles and the metadata contained within belong to the publishers; they are not ours to redistribute. We have a right to scrape and analyse them, but not to share them further without the publishers’ authorisation. Now… if Frontiers authorised us to release the web-scraped data that we collected from them, we could do that immediately.\nBut!!! While the raw data are not available, we are free to disclose analyses carried out on them. In this regard, we have an exciting announcement: we’ve just released a web app (in beta) that lets you explore data that we can share in a fully-customizable way! See the announcement here."
  },
  {
    "objectID": "posts/response_to_frontiers/index.html#straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said",
    "href": "posts/response_to_frontiers/index.html#straw-men-or-what-we-actually-said-vs.-what-frontiers-claims-we-said",
    "title": "Response to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”",
    "section": "2. Straw men, or: what we actually said vs. what Frontiers claims we said",
    "text": "2. Straw men, or: what we actually said vs. what Frontiers claims we said\nFrontiers claims we said many things we simply never said. We encourage readers to actually read our manuscript to see what it is we do say. Below we give a brief summary of some of the many straw men populating Frontiers’ piece.\nSampling of Frontiers statements:\n\n“The study posited that the scientific community is under strain due to a declining workforce and an exponential increase in published articles, caused by special issues in open access journals.”\n“Attributing the growth of scientific output solely to gold open access publishers […].”\n“It is clearly flawed to single out the shift of academic publishing towards open access as the sole driver of increase in scientific output […].”\n\nAlmost all of that is a straw man. \n\nWe don’t say the workforce is declining, we say (in the Abstract): “Total articles… have grown exponentially… which has outpaced the limited growth – if any – in the number of practising scientists.”\nWe never attribute the growth to a single factor – the very aim of our study is to propose five indicators of strain covering different factors. Also, (from the discussion): “The strain we characterise is a complicated problem, generated by the interplay of different actors in the publishing market”.\nWe never attribute the growth solely to gold OA publishers. Instead, we highlight at least two broad models, stating (in the Discussion): “the amount of strain generated through these two strategies is comparable”; we indicate several problems over our five indicators. In fact: we openly and clearly write this (from the Discussion) “regulating behaviours cannot be done at the level of publishing model. Gold open access, for example, does not necessarily add to strain [...].”\nThese are two things we do say though:\n\nThat growth in yearly published papers is exponential, and\nThat the number of researchers has not kept up with that growth\n\nFrontiers claims these are “false pillars of strain”, suggesting instead that the growth in articles we described is linear, and that the number of scientists over time has seen “a continuous increase.” We will give these suggestions the scrutiny they deserve.\nLinearity\nFirst, it bears saying: “Humans tend to systematically underestimate exponential growth and perceive it in linear terms” (Frontiers in Psychology, 2023). With this in mind… Frontiers suggested growth in articles was linear by… saying so. While criticizing us for not performing “scientific analysis”, their analysis can be summed up as showing plots and saying “it’s linear!”\n\nHowever, we ran the stats on this, and over our study’s time period, a linear model is a good fit (R2 = 0.93) but an exponential model is an even better fit (R2 = 0.97, below). Indeed, we reported a mean year-over-year growth of ~5.6% over the period 2016-2022. The bizarre thing is… Frontiers also annotated a 6% year-over-year growth in their Dimensions data from 2017-2022 (above: see the Dimensions data they shared).\nConstant absolute growth is linear. Constant year-over-year percent growth is exponential. So in fact, we both agree? The data are exponential. Below we have provided plots of our data with linear or exponential curves annotated, and provided formulae and model fits for both models.\n\n\n\nWho wore it better? Linear (red), or exponential (black)?\n\n\n\n\n“Humans tend to systematically underestimate exponential growth and perceive it in linear terms, which can have severe consequences in a variety of fields.”  - Melnik-Leroy, Gerda Ana, et al. “Is my visualization better than yours? Analyzing factors modulating exponential growth bias in graphs.” Frontiers in psychology 14 (2023): 1125810.\n\n\n\n\n\n\n\n\n\n\nModel\n2013-2022\nR2\n2000-2022\nR2\n\n\nLinear: ax + b\nax = 120379 * yr\nb = -240712967\n0.93\na = 80319 * yr\nb = -159895398\n0.96\n\n\nExponential: abx\na = 1.637*106\nbx = (5.76* 10-2) (yr - 2013)\n0.97\na = 8.74*105\nbx = (5.12*10-2) (yr-2000)\n0.99\n\n\n\nDecline of the number of researchers\nAgain, we never claimed a decline. From our abstract: “Total articles… have grown exponentially… which has outpaced the limited growth – if any – in the number of practising scientists.”\nSome of our plots do show a decline in the number of new PhDs per year. A lower growth rate does not create an absolute reduction – as people all over the planet facing slower inflation this year surely know. Further, we did consider different data types and sources to pin down the growth in the global scientific workforce: some show a plateau, some show only a limited rate of growth. That’s why we said “growth in articles outpaced the limited growth – if any – of researchers”. It’s what the data (from several sources) tell us. Here is our Fig.1supp1, which uses OECD PhD data supplemented with data for India and China (A-B), or UNESCO data on researchers-per-million (C-D):\n\n\n\nFig. 1supp1 from The Strain on Scientific Publishing: the growing disparity between total articles per year and active researchers is robust to use of alternate datasets. Dotted lines indicate estimated trends. A) OECD data complemented with total STEM PhD graduates from India and China (dashed red line) does not alter the pattern of an overall decline in recent years. B) The ratio of total articles to total PhD graduates has gone up substantially since 2019. C) UNESCO data instead using total active researchers (full-time equivalent) per million people shows a similar trend. Of note, this proxy for active researchers may include non-publishing scientists (private industry, governmental) that are not participating in the strain on scientific publishing in the same way that academic scientists are. D) Nevertheless, using UNESCO data the ratio of total articles to total active researchers has gone up substantially since 2019.\n\n\nSo, what did we actually say?\nWe claim, and document, that strain is a real problem, and while it’s not a new problem, it’s become seriously overwhelming in the last few years. We explicitly discuss how two main mechanisms have generated this strain: i) A steady growth in total journals, and in articles per journal, by legacy publishers like Elsevier and Springer. ii) An explosion of articles by publishers adopting special issues as a format to publish the majority of their articles, such as MDPI and Frontiers. In our discussion, we further emphasize these two mechanisms as distinct contributors to strain, and at no point do we suggest that strain is caused by a single publisher or a single publishing behaviour.\nStrain is caused by an industry that seeks to grow off the backs of a volunteer workforce that increasingly cannot keep up with demand to write, review, and edit new articles. Read that again and think of your life as a scientist. It resonates, right? What we did is characterize the constitution of that strain, synthesising data that are rarely considered collectively.\nFinally, the straw man to bind them all\nIn their piece, Frontiers makes us out to somehow be “detractors” of the open science movement, setting out to falsely prove our pre-ordained views… while discussing our open access preprint. We are confident our record of public statements and publishing history firmly advocate for open science."
  },
  {
    "objectID": "posts/response_to_frontiers/index.html#of-bad-bibliometrics-that-dont-add-up",
    "href": "posts/response_to_frontiers/index.html#of-bad-bibliometrics-that-dont-add-up",
    "title": "Response to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”",
    "section": "3. Of Bad bibliometrics that don’t add up",
    "text": "3. Of Bad bibliometrics that don’t add up\nWe struggled to come up with a delicate way to phrase this, but we simply couldn’t find the words. To be blunt: the Frontiers analysis is amateurish, careless in its data curation and interpretation, leans heavily on visual impressions, and evokes results out of thin air. Let’s dive in.\nFrontiers claims:\n“The proxy data was not reproducible and not representative of the original sources of Scopus and Web of Science. The original data does not show the claimed”exponential increase” in the total number of articles published during the study period (2013-2022), in fact growth is linear during this period (Figure 2A).”\nFirst: Frontiers presents this plot of exponential historical article growth, which suggests that the total new articles per year has declined from 2021 to 2022 slightly in Dimensions and Scopus, and by approximately half a million articles per year in the Web of Science. Here is their Figure 1:\n\nHowever, in their next plot in their Figure 2A (below, left), they instead show a year-over-year increase in the Dimensions and Scopus datasets, and only a minor decline in the Web of Science data. What Frontiers has done in Figure 1 is they have oversmoothed their curve, leading to a faulty data visualization that does not accurately reflect the underlying data.\nTheir Figure 2A also shows another problem with the analysis carried out by Frontiers, that will occur again later: they claim one thing in the text, provide no numbers in support, and simply reference a figure that, in fact, does not support the claim.\nLook at the left panel below, and then the right panel. Frontiers claims that our data are “incomplete and selective” and that “employing reproducible data from original, verifiable sources painted a starkly different picture”. They then provide their Figure 2A, which we give here unaltered in the left panel below. In the right panel, we have cut and pasted the black line (our data) and moved it up so the 2013 data points are aligned between Scopus and “our data”. The growth rate is remarkably similar. This should not surprise anyone, because our data are Scopus data. We just filtered them to only include journals also indexed in Web of Science. They are not a “proxy, unverifiable source.”\n\nBut to be sure we do not somehow mislead you with a trick of data visualisation, we crunched the numbers: the correlation between our data and Scopus is: Pearson’s r = 0.992. In fact, the correlation between our data and the Dimensions data Frontiers provided us is: Pearson’s r = 0.983. So even if you use Dimensions or Scopus instead, our data do paint the same picture.\nPillars made of soapstone: another example of claiming a result, not running any analysis, and then reaching a conclusion not supported by the figure is their “second pillar of strain”: the limited growth – if any – in the number of researchers. Here is Frontiers’ claim:\n“The study uses PhD graduates as a proxy for active researchers who write, review, and edit articles when a more direct measure of active researchers is available… They ignore the upward trend available from the same data source they used (Figure 2B) and fail to mention or consider that PhD graduations were disrupted during the pandemic. A direct measure of the number of active researchers shows the opposite to what they claim – a continuous increase.”\nAgain, we never say there has been a decrease in researchers (see above). Frontiers then provides their Figure 2B and claims it shows a continuous increase in researchers.\n\n\n\n\n\n\nBut we tend to see a plateau in this plot, similar to the plateau we saw in newly-minted PhDs. And again, the reduction in newly-minted PhDs was robust to inclusion of data from India and China (see Fig.1supp1 above). We further double checked this trend using UNESCO data on researchers per million, similar to what Frontiers has done here, finding a very limited growth in recent years. That’s why we only claimed that article growth had outpaced the growth - if any - in active researchers.\n\n\nThere are actually additional problems with the data that Frontiers plot here: the long timeframe they show is misleading because the countries indexed in the OECD dataset have evolved over time. Indeed, the huge spike in 2013 is due to the inclusion of many new countries, including Germany, explaining this large spike in total new researchers.\nFurther, Frontiers claims the COVID-19 pandemic is behind the plateauing of PhD graduates that we saw. However, this plateau, also seen in the total researchers data above, started in ~2017. So we can’t exactly attribute this to the pandemic. So again, none of what Frontiers claims is true: there has not been a continuous increase in total researchers, but rather the same plateau we saw in PhD graduate numbers - the agreement between these two metrics is also quite sensible, considering PhD graduates become researchers.\nFrontiers actually validates our data: this response would not be complete without a look at Figure 2C. It purports to show the absence of correlation between total articles per year being generated and the number of Frontiers articles published in “Research Topics” (i.e. special issues). Frontiers plots the datasets of Scopus and Web of Science as lines, and then its own Research Topic articles as very small bars, choosing two different visualization styles within the same plot.\n\n\n\n\n\n\nBriefly: there is actually quite a good correlation between total articles and Frontiers Research Topic articles per year. The correlation between the Scopus data in green and the Frontiers Research Topic data is: Pearson’s r = 0.912. Moreover, the growth in Frontiers Research Topic articles has been exponential: a mean year-over-year growth of 43%, far outpacing the overall strain (~6%).\nBut somewhat ironically: this plot actually validates our own scraped data from Frontiers. As we mentioned, we requested data from Frontiers on special issue article numbers so we could validate or even replace our web-scraped data, but they never sent us those data. But in this Figure 2C, in fact, Frontiers provides the data we requested. So, how did we do? It seems we were spot-on.\n\n\nOur web-scraped dataset estimated 69.3% of articles in Frontiers were published through “Research Topics” (i.e. special issues). Applying this estimate to their 2022 output (125.1k articles) yields ~87k special issue articles in 2022 – the Figure here says 88k. Thus, our scrape of Frontiers articles yielded ~99% accuracy – pretty impressive given we only scraped the 49 journals from our study, while the 88k reflects all 200+ Frontiers journals. Thank you to Frontiers for providing these data that validates our web-scraping."
  },
  {
    "objectID": "posts/response_to_frontiers/index.html#conclusion",
    "href": "posts/response_to_frontiers/index.html#conclusion",
    "title": "Response to: “Bad bibliometrics don’t add up for research or why research publishing policy needs sound science”",
    "section": "Conclusion",
    "text": "Conclusion\nWe started a conversation with Frontiers while we were working on our article. We offered them the chance to comment on our work before we released it publicly. This is a courtesy we also extended to other publishers. We were thus surprised to find this blog posted without our knowledge. Indeed, we had numerous emails with Frontiers in the lead-up to our work that were good-faith exchanges. We even have the original comments that Frontiers provided to us from when we sent them our draft article: they praised aspects of our work and provided constructive feedback, including correcting our phrasing to avoid ambiguous wording. We would be happy to share those comments, if Frontiers would give us permission to. We still thank Frontiers for those comments. \nThat is why it is so surprising to see this latest piece, which contains a startling level of animosity and many derogatory accusations that are simply untrue.\nOthers, including publishers, have welcomed our scientific countribution. Frontiers could have done the same. Instead, Frontiers has released this blog containing many analytical errors just one week after they published the questionable article on rodent genitalia. Of course mistakes happen, and Frontiers retracted that article swiftly. But this? This is something else. These are their own words, chosen intentionally. \nWe still want to thank specific employees at Frontiers with whom we had a respectful relationship. We made every effort we could to adhere to ethical research conduct. We had good faith, mutually beneficial exchanges with Frontiers. Thus why it is so disheartening to see this blog post and the tone it took. We hope Frontiers will reflect on what they’ve said, how they’ve said it, and choose to engage with us more productively in the future.\nWe wrote the Strain paper because we wanted there to be more transparency over the data that publishers control on academic publishing; because we thought we needed a more strongly data-driven conversation about publishing trends; because some of the contributions to that conversation are misguided, dysfunctional and require rectifying, and because we think researchers need to scrutinise publishers’ data if they are to be effectively analysed and interpreted. We cannot assume publishers will do that well.\nThe Frontier’s blog confirms us in that view.\nSincerely,\nMark A. Hanson, Pablo Gómez Barreiro, Paolo Crosetto, Dan Brockington"
  }
]